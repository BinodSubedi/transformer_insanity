{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "febc066d-604c-4486-929c-4467b7515366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2095493-fa37-4bd0-b4b0-deb0b2b5eb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c58a107-6856-4e81-b384-6da35dc8c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ',', '.', '\\n', '?', '!', \"'\", ':', ';', '--', '3', 'A', 'ABHORSON', 'ABRAHAM', 'ADRIAN', 'AEacides', 'AEdile', 'AEdiles', 'AEneas', 'AEsop', 'ALL', 'ALONSO', 'ANGELO', 'ANNE', 'ANOTHER', 'ANTIGONUS', 'ANTONIO', 'ARCHBISHOP', 'ARCHIDAMUS', 'ARIEL', 'AUFIDIUS', 'AUMERLE', 'AUTOLYCUS', 'Abase', 'Abate', 'Abated', 'Abbot', 'Abel', 'Abhorred', 'Abhorson', 'Abides', 'Able', 'About', 'Above', 'Abraham', 'Absolute', 'Accept', 'Accomplish', 'According', 'Accords', 'Account', 'Accountant', 'Accursed', 'Accuse', 'Achieve', 'Acquaint', 'Action', 'Adam', 'Add', 'Added', 'Adding', 'Address', 'Adieu', 'Adjudged', 'Admit', 'Adonis', 'Adoptedly', 'Adopts', 'Adrian', 'Adriatic', 'Advance', 'Advantaging', 'Adversity', 'Advertising', 'Advocate', 'Affection', 'Affliction', 'Affrighted', 'Affrights', 'Affront', 'Afore', 'Afresh', 'Afric', 'African', 'After', 'Again', 'Against', 'Agamemnon', 'Age', 'Aged', 'Agenor', 'Agreed', 'Agrippa', 'Ah', 'Aim', 'Aiming', 'Airy', 'Ajax', 'Al', 'Alack']\n"
     ]
    }
   ],
   "source": [
    "def text_split_assist(text):\n",
    "    comma_splitted = ' '.join(text.split(','))\n",
    "    newLine_split = ' '.join(comma_splitted.split('\\n'))\n",
    "    # remaining are removing &c, :, --, ', ?, !, ., ;\n",
    "    colon_split = ' '.join(newLine_split.split(\":\"))\n",
    "    double_dash_split = ' '.join(colon_split.split('--'))\n",
    "    kotation_split = ' '.join(double_dash_split.split(\"'\"))\n",
    "    question_mark_split = ' '.join(kotation_split.split('?'))\n",
    "    exclamation_split = ' '.join(question_mark_split.split('!'))\n",
    "    fullstop_split = ' '.join(exclamation_split.split('.'))\n",
    "    semiColon_split = ' '.join(fullstop_split.split(';'))\n",
    "    garbage_removable_split = ' '.join(semiColon_split.split('&C'))\n",
    "    final_split = ' '.join(garbage_removable_split.split('&c'))\n",
    "    return final_split\n",
    "\n",
    "final_split = text_split_assist(text)\n",
    "\n",
    "total_tokens = sorted(set(list(final_split.split(' '))))\n",
    "# Adding all the unique necessary elements we removed\n",
    "unique_elements = [' ',',','.','\\n','?','!', \"'\",':',';','--']\n",
    "# print(total_tokens[len(total_tokens) - 1])\n",
    "# to remove '' token\n",
    "total_tokens.pop(0)\n",
    "total_tokens = unique_elements + total_tokens\n",
    "print(total_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4bc1c51-07fa-4c80-a539-6c4569dd6cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13851\n",
      "[22, 8, 0, 9]\n",
      "ANGELO; --\n"
     ]
    }
   ],
   "source": [
    "#very simple tokenizer\n",
    "clean_text1 = ' '.join(text.split('&C'))\n",
    "final_text = ' '.join(clean_text1.split('&c'))\n",
    "# print(final_text[:100])\n",
    "\n",
    "import re\n",
    "\n",
    "def encode_text_assist(plain_text):\n",
    "    pattern = r\"(--|[,.\\n?!':;\\s])\"\n",
    "    plain_text = re.split(pattern, plain_text)\n",
    "    #joining with spaces\n",
    "    # plain_text = ' '.join(plain_text)\n",
    "    #now splitting through spaces\n",
    "    # plain_text = plain_text.split(' ')\n",
    "    tokens_arr = []\n",
    "    for i,t in enumerate(plain_text):\n",
    "        # print(f't:{t}')\n",
    "        # t = t.strip()\n",
    "        if t and t in total_tokens:\n",
    "            tokens_arr.append(t)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return tokens_arr\n",
    "\n",
    "vocab_size = len(total_tokens)\n",
    "word_to_int = {w:i for i,w in enumerate(total_tokens)}\n",
    "int_to_word = {i:w for i,w in enumerate(total_tokens)}\n",
    "# in encode I need to create some function to split up words\n",
    "encode = lambda l: [word_to_int[w] for w in encode_text_assist(l)]\n",
    "encode_list = lambda l: [word_to_int[w] for w in l]\n",
    "decode = lambda l: ''.join([int_to_word[i] for i in l])\n",
    "print(vocab_size)\n",
    "print(encode(\"ANGELO; --\"))\n",
    "print(decode(encode(\"ANGELO; --\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00b85913-35e1-48c1-b356-4ed419f1b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Perceptron/neuron Node, Layer and MLP Class\n",
    "# class Neuron:\n",
    "#     def __init__(self,prev_layer,forward_layer,activation,embedding_per_vocab):\n",
    "#         # self.prev_layer = prev_layer\n",
    "#         # self.forward_layer = forward_layer \n",
    "#         self.activation = activation\n",
    "#         self.initial_random_w_gen()\n",
    "#         self.embedding_per_vocab = embedding_per_vocab\n",
    "        \n",
    "        \n",
    "\n",
    "#     # random weight generation for initial setup\n",
    "#     def initial_random_w_gen(self):\n",
    "#         self.weight = [self.random_param_gen() for _ in range(self.embedding_per_vocab)]\n",
    "\n",
    "#     # Instead of creating bias per neuron, we will create biases arrays per layer, which just makes life easier working with matrix operations\n",
    "#     # adding biases to the final mat mul of weights and input values\n",
    "#     # def initial_bias_gen(self):\n",
    "#     #     self.bias = self.random_param_gen()\n",
    "        \n",
    "#     def random_param_gen(self,range = 8):\n",
    "#         return np.random.uniform(-range,range)\n",
    "\n",
    "#     def apply_relu(self,val):\n",
    "#         # relu simply means, if negative or zero, the value is 0 and if positive, y = x\n",
    "#         np.maximum(0,val)\n",
    "\n",
    "#     def forward_pass(self):\n",
    "#         pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df84f317-20dc-4b0f-a7dc-a14fcf8ad603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting softmax on global scope, as it is used in many clases\n",
    "# def temperature_scaled_softmax(logits, T=1.0):\n",
    "#     logits = np.array(logits)  # Ensure it's a NumPy array\n",
    "#     scaled_logits = logits / T  # Apply temperature scaling\n",
    "#     exp_values = np.exp(scaled_logits - np.max(scaled_logits))  # Stability trick\n",
    "#     return exp_values / np.sum(exp_values)\n",
    "\n",
    "def softmax(x, dtype=np.float32):\n",
    "    # \"\"\"Numerically stable softmax with float32 or float64 support.\"\"\"\n",
    "    x = np.asarray(x, dtype=dtype)  # Ensure correct dtype\n",
    "    x_max = np.max(x, axis=-1, keepdims=True)  # Stability trick\n",
    "    exp_x = np.exp(x - x_max, dtype=dtype)  # Compute exp with correct dtype\n",
    "    print(f'softmax_valllll:{np.sum(exp_x, axis=-1, keepdims=True, dtype=dtype)}')\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True, dtype=dtype)\n",
    "\n",
    "    # I need to use adaptive scaling strategy, where I initially use average probaility measuring technique and when the output variation increases more\n",
    "    # I will opt to using real softmax\n",
    "    #Normalizing negative values to 0 by adding smallest value positive alternate to zero down\n",
    "    \n",
    "    # smallest_element = abs(min(x)) # abs to convert to absoulte positive value\n",
    "    # equalizing_negative_val = x + np.array([smallest_element])\n",
    "    # sum_val = np.sum(equalizing_negative_val, axis=0)\n",
    "    # normalized_val = np.array(equalizing_negative_val / sum_val, dtype=dtype)\n",
    "    # return normalized_val\n",
    "\n",
    "    # return temperature_scaled_softmax(x,100)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nodes_num,embedding_per_vocab=None, bias = True,prev_layer = None, forward_layer = None,activation = 'relu',type='hidden'):\n",
    "        self.nodes_num = nodes_num\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.prev_layer = prev_layer\n",
    "        self.forward_layer = forward_layer\n",
    "        self.type = type\n",
    "        # self.bias = bias\n",
    "        self.embedding_per_vocab = embedding_per_vocab\n",
    "        # We could choose some layer without any bias\n",
    "        if bias:\n",
    "            self.initial_bias_gen(range=2)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.create()\n",
    "\n",
    "    def initial_bias_gen(self,range):\n",
    "        self.biases = self.__gen_random_numbers(self.nodes_num,range)\n",
    "    \n",
    "    def __gen_random_numbers(self,size,range):\n",
    "        #not the best way, but it works starting out with any random number\n",
    "        # return range * np.random.rand(size)\n",
    "        # so, we use something better like:\n",
    "        return np.random.uniform(-range,range,size=size)\n",
    "\n",
    "    def apply_relu(self,val):\n",
    "        # relu simply means, if negative or zero, the value is 0 and if positive, y = x\n",
    "        np.maximum(0,val)\n",
    "    \n",
    "    def create(self):\n",
    "        # Instead of explicitly Making another Neuron class and creating segmentation\n",
    "        # I would use the better option to create matrix of weights per node per layer, and also, simd is just better usable this way\n",
    "        # self.nodes = [Neuron(self.prev_layer,self.forward_layer,self.activation,embedding_per_vocab=self.embedding_per_vocab) for _ in range(self.nodes_num)]\n",
    "        self.nodes = [self.__gen_random_numbers(self.embedding_per_vocab,range=8) for _ in range(self.nodes_num)] if self.type == 'input' else [\n",
    "            self.__gen_random_numbers(self.prev_layer.nodes_num,range=8) for _ in range(self.nodes_num)]\n",
    "\n",
    "    # Receive the data processed by previuos MLP layer or if the layer is first, receives the output from mskd-self-attention layer\n",
    "    def forward_pass(self,input_arr):\n",
    "        element_wise_weight_mmul = np.array(self.nodes) * np.array([input_arr])\n",
    "        mmul_val_addition = np.sum(element_wise_weight_mmul, axis=1)\n",
    "        if(self.bias == None):\n",
    "            return mmul_val_addition\n",
    "        else:\n",
    "            bias_addition = np.array(mmul_val_addition) + np.array(self.biases)\n",
    "            # return processed vale with weights and input arr\n",
    "            return bias_addition\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccaab3c7-ad2a-4099-af9c-52908d32acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers_num, nodes_per_layer,embedding_per_vocab,custom_layer_config={}):\n",
    "        self.layers_num = layers_num\n",
    "        self.nodes_per_layer = nodes_per_layer\n",
    "        self.embedding_per_vocab = embedding_per_vocab\n",
    "        self.mlp_allLayers_values = []\n",
    "        self.initiate()\n",
    "\n",
    "    # def initiate(self):\n",
    "    #     self.head = Layer(self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab)\n",
    "    #     current_layer = self.head\n",
    "    #     for _ in range(self.layers_num - 1):\n",
    "    #         new_layer = Layer(nodes_num=self.nodes_per_layer)\n",
    "    #         current_layer.forward_layer = new_layer\n",
    "    #         new_layer.prev_layer = current_layer\n",
    "    #         current_layer = new_layer\n",
    "\n",
    "    # In future we are going to create matrix for weights of each layer and each neuron inside it, which will be better approach then using linked-lst\n",
    "    # style linking layers.\n",
    "    def initiate(self):\n",
    "        #this is the array implementation of layers and neurons mapping, which is better than using above lnkd-lst method\n",
    "        # we did if else here because in the layer class, we need to be able to distinguish between layer types and position\n",
    "        # to assign specific weights shape\n",
    "        # self.layers = [Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,type='input') if i==0 else\n",
    "        #                Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,prev_layer=self.layers[i-1])\n",
    "        #                for i in range(self.layers_num)]\n",
    "\n",
    "        # self.layers = [Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,prev_layer=self.layers[i-1])\n",
    "        #                for _ in range(self.layers_num)]\n",
    "        \n",
    "        # self.layers[0] = Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,type='input')\n",
    "\n",
    "        # Neither of above way worked as, self.layers[i - 1] couldn't be assigned sooo...\n",
    "        self.layers = []\n",
    "        for i in range(self.layers_num):\n",
    "            if i == 0:\n",
    "                self.layers.append(Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,type='input'))\n",
    "            else:\n",
    "                self.layers.append(Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,prev_layer=self.layers[i-1]))\n",
    "        \n",
    "    # here we will attach the self attention masking layer\n",
    "    ## Or not, cause we could just use model class to indirectly create propagation between MLP and self attention layers or any\n",
    "    # def attach_to_head(self, attention_layer):\n",
    "            \n",
    "    def feed_forward(self,masked_self_attn_embedd_arr):\n",
    "\n",
    "        # Removing any data that existed before\n",
    "        # I opted to keep data according to data processed in matrix form\n",
    "        # making easier for final averaging gradient on position basis\n",
    "        self.mlp_allLayers_values = []\n",
    "        \n",
    "        #Now element wise matrix mul with weights of neurons in layer and mskd_self_attn_embedd_arr val, one layer at a time, and\n",
    "        # also adding bias at the end of neuron wise mat mul\n",
    "        data_to_process = masked_self_attn_embedd_arr\n",
    "        for layer in self.layers:\n",
    "            # this add all the values output from slef-attn layer output adjacent input to second-last layer, as at last layer the loop ends\n",
    "            self.mlp_allLayers_values.append(data_to_process)\n",
    "            data_to_process = layer.forward_pass(data_to_process)\n",
    "\n",
    "        # Adding last layer or the layer before normalization layer\n",
    "        self.mlp_allLayers_values.append(data_to_process)\n",
    "        # print(f'length of values assigned to corresponding layer::{len(self.mlp_allLayers_values)}')\n",
    "        # returns the arr after getting operated through all MLP layers\n",
    "        # now Model class will pass this return value to normalizer for getting probability distribution out of it\n",
    "        return data_to_process, self.mlp_allLayers_values\n",
    "        \n",
    "        \n",
    "# layer = Layer(10)\n",
    "# for n in layer.nodes:\n",
    "#     print(f'n.weight: {n.weight}')\n",
    "#     print(f'n.bias: {n.bias}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "067b4c62-60bc-4898-b066-32abbe3c06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttLayer:\n",
    "    def __init__(self,context_len=8,initial_base_f =20,positional_function_f_change = 1.5,embedding_len_per_vocab = 2):\n",
    "        self.context_len = context_len\n",
    "        # it will look like: [ [[query final value],[key final value],[value final value],[embedding at the last, that is after adding self-attn value]],\n",
    "        #[---same as before---], [---same as before---] ]\n",
    "        self.context_history = {}\n",
    "        # this means, for vector, each index gets uplift of 30, like 30, 60, 90, as f increases like x2, it will increase to 60, 120, 180 and so on\n",
    "        self.base_frequency = initial_base_f \n",
    "        self.frequency_change_rate = positional_function_f_change\n",
    "        self.embedding_per_vocab = embedding_len_per_vocab\n",
    "        self.gen_first_weights()\n",
    "        self.gen_initial_query_key_value_w()\n",
    "    \n",
    "    #start input propagation\n",
    "    def start_input_prop(self,data,mode='train'):\n",
    "        # In train mode, we need to train the layers for guessing every next token,\n",
    "        # but in trained model, we only use other token to get masked self-attention value for very last token, and then run through MLP\n",
    "        inference_len = len(data)\n",
    "        # we say inference_len because we will always get complete 8 tokens for training\n",
    "        data_np = np.array([data])\n",
    "        # eg [[1,2],[3,4]] * [3,4] = [[3,8],[9,16]]\n",
    "        initial_values = np.array(self.first_weights) * data_np\n",
    "        # Adding positional encodings, sine cosine alternating function values\n",
    "        self.setup_main_self_attn_work(initial_values)\n",
    "        \n",
    "    def setup_main_self_attn_work(self,init_values):\n",
    "        # Here we itteratively create context_len values and then try to train the MLP layer # I am still tinkering so, might not be correct\n",
    "        for i in range(self.context_len):\n",
    "            #setting up positional encoding(means adding pos. encoding for every context_len vals)\n",
    "            for j in range (self.embedding_per_vocab):\n",
    "                # so essentially i is token number and j is the embedding number\n",
    "                # we did i+1 and j+1 cause, the alternating function takes position that starts with 1 not 0 like indexing in code\n",
    "                init_values[i][j] += self.alternating_sine_cosine_p_value(i+1, j+1)\n",
    "        # after adding positional_values\n",
    "        self.after_positional_values = init_values\n",
    "        # now multiplying to get query, key and values parameter context wise value\n",
    "        self.context_history['query'] = self.after_positional_values * np.array(self.query_weights)\n",
    "        self.context_history['key'] = self.after_positional_values * np.array(self.key_weights)\n",
    "        self.context_history['value'] = self.after_positional_values * np.array(self.value_weights)\n",
    "            \n",
    "\n",
    "    #generate initial weight for the very first layer of weights for inputs, we are only using two size vector for data representation\n",
    "    #with weights for each context_len assigned value, means each vector contains context_len(default 8) weights\n",
    "    def gen_first_weights(self):\n",
    "        self.first_weights = [self.__gen_random_numbers(self.context_len) for _ in range(self.embedding_per_vocab)]\n",
    "        \n",
    "        \n",
    "    #generate initial random query, key and value weights\n",
    "    def gen_initial_query_key_value_w(self):\n",
    "        # initializing the weights for Q,K and V\n",
    "        #For multi-head, I don't understand that completely now, but we will come back to it\n",
    "        # self.query_weights = [self.__gen_random_numbers(self.embedding_per_vocab) for _ in range(self.context_len)]\n",
    "        # self.key_weights = [self.__gen_random_numbers(self.embedding_per_vocab) for _ in range(self.context_len)]\n",
    "        # self.value_weights = [self.__gen_random_numbers(self.embedding_per_vocab) for _ in range(self.context_len)]\n",
    "\n",
    "        # For sigle head self-attn layer\n",
    "        self.query_weights = [self.__gen_random_numbers(self.embedding_per_vocab)]\n",
    "        self.key_weights = [self.__gen_random_numbers(self.embedding_per_vocab)]\n",
    "        self.value_weights = [self.__gen_random_numbers(self.embedding_per_vocab)]\n",
    "        \n",
    "\n",
    "    def __gen_random_numbers(self,size):\n",
    "        #not the best way, but it works starting out with any random number\n",
    "        return 2 * np.random.rand(size)\n",
    "\n",
    "    #We need alternating sine cosine position value for each token, but as the frequency increase per couple and need for specific number of couples are\n",
    "    #dependent upon the size of network, we need to set that up\n",
    "    def alternating_sine_cosine_p_value(self,token_position,vector_position):\n",
    "        # token position is like 2 in hello there for there\n",
    "        # vector position is the position of value in first index of first token value\n",
    "        couple_number = math.ceil(vector_position / 2)\n",
    "        # we do (couple_number -1) * frequency change rate, because we don't just multiply by couple number, we have a set frequency increase rate\n",
    "        relative_f_change = couple_number if couple_number == 1 else (couple_number - 1) * self.frequency_change_rate\n",
    "        # we alternate between sine and cosine periodically\n",
    "        sine_or_cosine = 'sine' if vector_position % 2 == 0 else 'cosine'\n",
    "        if sine_or_cosine == 'sine':\n",
    "            return np.sin(np.radians(token_position * (self.base_frequency * relative_f_change)))\n",
    "        else: \n",
    "            return np.cos(np.radians(token_position * (self.base_frequency * relative_f_change)))\n",
    "            \n",
    "    # As the q, k, and v might be needed to be remembered migh as well use MLP class here to give the final matrix\n",
    "    # which should be normalized by passing through final linear layer and get probability distribution\n",
    "    # I don't know the exact way I am going to do, we will tinker and see what works best\n",
    "    # what I am doing here is sending back the whole batch initial embedd ready to be trained one array at a time\n",
    "    def batch_feed_forward_data_ready(self,token_input_arr):\n",
    "        # Passing whole single batch data, means matrix of shape (batch_size * context_len) \n",
    "        # so, weights of every array of batch, multiplies with corresponding weight index/number all at once\n",
    "        token_input_arr = np.array(token_input_arr)[:,np.newaxis,:] * np.array(self.first_weights) # here first_weights are broadcasted to each batch_arr\n",
    "        token_input_arr = token_input_arr.transpose(0,2,1) # we are trying to keep embedding of one voab in single array stream\n",
    "        return token_input_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "53fd07c7-57ee-4a6f-8580-01403d521aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will be the class we would use, which wraps all inner workings with all the previous classes\n",
    "class Model:\n",
    "    def __init__(self,context_len,training_batch_number=None,embedding_len_per_vocab=2):\n",
    "        self.context_len = context_len\n",
    "        self.embedding_per_vocab = embedding_len_per_vocab\n",
    "        self.training_batch_number = training_batch_number\n",
    "        self.attention_layer = MaskedSelfAttLayer(context_len=self.context_len,embedding_len_per_vocab=2)\n",
    "        self.mlp = MLP(nodes_per_layer=12,layers_num=6,embedding_per_vocab=self.embedding_per_vocab)\n",
    "        # Will have error per batch\n",
    "        self.err_per_batch_tokens =  np.zeros(vocab_size)\n",
    "        self.mlp_all_layers_values = []\n",
    "        # This final normalizing layer is used to decrease the parameters of the whole network, rather we use cont_len nodes at the final_layer\n",
    "        # which was incorrect, because we need to create probability distribution over all the possible tokens \n",
    "        # so, final layer will have len(total_tokens) which is vocab_size\n",
    "        self.final_normalizing_layer = Layer(nodes_num=vocab_size,bias=False,prev_layer=self.mlp.layers[self.mlp.layers_num - 1])\n",
    "\n",
    "\n",
    "    def negative_log_loss_penalty(self,ground_truth,predicted_value):\n",
    "        lower_limit = 0.0000001\n",
    "        arr = np.clip(predicted_value,lower_limit, 1-lower_limit)\n",
    "        neumerator = arr - ground_truth\n",
    "        denominator = arr * (np.ones(vocab_size) -  arr)\n",
    "        return neumerator / denominator\n",
    "        \n",
    "\n",
    "\n",
    "    # Need to create Gradient descent values for all of these layers\n",
    "    # calculating the value change that would propagate from each node to the final output, and how changes in input change final output\n",
    "    def backpropagation(self):\n",
    "        #calculate gradient for each layer by muliplying a from last layer to first a being a * w + b = f(x)\n",
    "        self.gradient_last_to_first_layer = []\n",
    "\n",
    "        # Now reversing the values of layers as we are calculating gradient from last to first\n",
    "        # from normalized layer to first layer\n",
    "        reversed_layerWise_values = self.average_per_layers_value[::-1]# clipped the last value\n",
    "\n",
    "        # print(reversed_layerWise_values)\n",
    "        # we leave the last layer value as it is only useful for getting the expected answer and penalty\n",
    "        # In backprop we only use values from second last layer\n",
    "        layers_reversed = self.mlp.layers[::-1]\n",
    "\n",
    "        # This is wrong, every node has different gradient\n",
    "        # Now revising the approach\n",
    "        for i,values_row in enumerate(reversed_layerWise_values):\n",
    "            # print(f'values_row::{reversed_layerWise_values}')\n",
    "            # here we do upto second layer as we don't need first layer values, as the first layer value is just output from\n",
    "            # self-attention layer\n",
    "            if i == 0:\n",
    "                self.gradient_last_to_first_layer.append(values_row)\n",
    "            else:\n",
    "                # gradient_last_to_first_layer[i-1] * values_row * numnber of nodes in layer before this\n",
    "                \n",
    "                # This is the wrong approach\n",
    "                # self.gradient_last_to_first_layer.append(self.gradient_last_to_first_layer[i-1] * values_row * layers_reversed[i - 1].nodes_num)\n",
    "\n",
    "                # Now improving to correct one\n",
    "\n",
    "                for index,val in enumerate(self.gradient_last_to_first_layer[i - 1]):\n",
    "                    self.gradient_last_to_first_layer.append(values_row * val * layers_reversed[i - 1].nodes_num)\n",
    "\n",
    "\n",
    "        # In this matrix first value of gradient is just for the extra nmormalized layer and all the other 72 array in base model is for\n",
    "        # other nodes in the MLP layers, every nodes weight with different/unique gradient\n",
    "        \n",
    "        print(f'gradients::::{self.gradient_last_to_first_layer}')\n",
    "        print(f'length of gradients::{len(self.gradient_last_to_first_layer)}')\n",
    "        # print(f'first_scaled_layer_grad::{self.first_scaled_layer_grad}')\n",
    "\n",
    "\n",
    "    # Update weight and biases, right now I am just updating weights, we will see about biases update later\n",
    "    def gradient_descent(self):\n",
    "\n",
    "\n",
    "        # Also print overall mean squared erro value here\n",
    "        pass\n",
    "    \n",
    "    # Creating a abstraction out of train, just to make code more readable and managable\n",
    "    def train_per_batch_arr(self,arr_data, expected_output):\n",
    "        self.attention_layer.setup_main_self_attn_work(arr_data)\n",
    "        batch_single_arr_after_norm = []\n",
    "        # Now everything is setup, now we need to serially go over token embeddings and multiply query with previous tokens key and create a softmax\n",
    "        # between its query key product value and other previous tokens value and aggregate to create final masked self attn-value\n",
    "        for i,_ in enumerate(arr_data):\n",
    "            \n",
    "\n",
    "            if(i == 0):\n",
    "                data_through_mlp, values_across_layers = self.mlp.feed_forward(self.attention_layer.context_history['value'][i]) # where i is always 0 in this situation\n",
    "            else:\n",
    "                # [:(i + 1)] because slicing is exclusive operation that leaves last index\n",
    "                query_key_mmul = np.array(self.attention_layer.context_history['query'][i]) * np.array(self.attention_layer.context_history['key'][:(i+1)])\n",
    "                # now adding each array into single number. eg:- [q2*k1 + q2*k1,q2*k2,q2*k2]\n",
    "                # By the way q2 is different in every instance like k1 and k2, i am just signifying the arr data relating to specific cont_len vocab\n",
    "                \n",
    "                # print(f'query_key_mmul:{query_key_mmul}')\n",
    "                softmax_ready_digits = np.sum(query_key_mmul,axis=1) # By the way axis=1 means adding row not column, 0 means column\n",
    "                # Now perform softmax\n",
    "                # print(f'sofmax_ready_digit::{softmax_ready_digits}\\n')\n",
    "                through_softmax = softmax(softmax_ready_digits)\n",
    "                # Now multiplying sofmax value corresonding to value of each cont_len vocab and add according to corresponding index\n",
    "                # print(f'values::{self.attention_layer.context_history['value'][:i+1]}\\n th_smax::{through_softmax[:,np.newaxis]}')\n",
    "                value_through_contx = self.attention_layer.context_history['value'][:i+1] * through_softmax[:,np.newaxis]# broadcasing through all array\n",
    "                final_summed_val = np.sum(value_through_contx, axis=0) # axis=1 because we need to add all vector embedding through corresponding index\n",
    "                # passing through MLP Network\n",
    "                data_through_mlp, values_across_layers = self.mlp.feed_forward(final_summed_val)\n",
    "\n",
    "            # Now passing through final_normalizing layer and push to batch_single_arr_after_norm arr and return back to train to delegate to some\n",
    "            # abstraction to handle remaining work\n",
    "            final_data = self.final_normalizing_layer.forward_pass(data_through_mlp)\n",
    "            # Also puhsing values across layers into self.mlp_allLayers_value so, that we could calculate average gradient per position and\n",
    "            # also pass through update chain to take multiple data units into consideration with average penalty\n",
    "            self.mlp_all_layers_values.append(values_across_layers)\n",
    "            # print(f'final_data_before_softmax::{final_data}\\n')\n",
    "            output_data = softmax(final_data)\n",
    "            \n",
    "                    \n",
    "            \n",
    "            # print(f'final_data:{final_data}\\n')\n",
    "            # print(f'first_index_val::{final_data[0]}')\n",
    "            batch_single_arr_after_norm.append(output_data)\n",
    "\n",
    "\n",
    "        # We will first calculate the negative log error for each value and then, average them all in a single dim array\n",
    "\n",
    "        for i,arr in enumerate(batch_single_arr_after_norm):\n",
    "            #Create a single dim array with zero initialized array of size vocab_size as we copare to ground_truth and predicted truth\n",
    "            zeros_arr = np.zeros(vocab_size)\n",
    "            # print(f'expected_output[i]__i::{len(arr)}\\n')\n",
    "            # print(f'expected_output[i]::{len(expected_output)}\\n')\n",
    "            zeros_arr[expected_output[i]] = 1\n",
    "\n",
    "            # Now Zeros_arr is the ground truth data arr and arr is predicted data arr, now we need to calculate negative log error for\n",
    "            # for each position\n",
    "            self.err_per_batch_tokens += self.negative_log_loss_penalty(zeros_arr,arr)\n",
    "            \n",
    "            # for j, GT in enumerate(zeros_arr):\n",
    "            #     self.err_per_batch_tokens[j] += self.negative_log_loss_penalty(GT, arr[j])\n",
    "\n",
    "        \n",
    "        # finally return all the batch_single array prediction through softmax\n",
    "\n",
    "        \n",
    "        # print(f'values_per layers in arr 8, len::{len(self.mlp_all_layers_values)}')\n",
    "        return batch_single_arr_after_norm\n",
    "            \n",
    "        \n",
    "    \n",
    "    # batch data and their respective expected output\n",
    "    def train(self,batch_data, batch_e_output,lr=0.001):\n",
    "        self.err_per_batch_tokens = np.zeros(vocab_size)\n",
    "        self.mlp_all_layers_values = []\n",
    "        batch_final_after_softmax = []\n",
    "        batch_predicted_token = []\n",
    "        batch_predicted_token_prob = []\n",
    "        batch_complete_data =self.attention_layer.batch_feed_forward_data_ready(batch_data)\n",
    "        # Now we loop through train_per_batch_arr as function name suggests, and finally get normalized data arr with probability distribution\n",
    "        for i,data in enumerate(batch_complete_data):\n",
    "            train_per_batch_val = self.train_per_batch_arr(data, batch_e_output[i])\n",
    "            for el in train_per_batch_val:\n",
    "                batch_predicted_token.append(np.argmax(el))\n",
    "                batch_predicted_token_prob.append(el[np.argmax(el)])\n",
    "            batch_final_after_softmax += train_per_batch_val\n",
    "\n",
    "        # dividing self.err_per_batch_tokens the number i.e. context_len * batch_size \n",
    "\n",
    "        self.err_per_batch_tokens /= (self.context_len * self.training_batch_number)\n",
    "\n",
    "        # Now averaging all the a value per layer, throughout the whole training process including normalizing layer\n",
    "        # print(f\"self.mlp_allLayers_value::{len(self.mlp_all_layers_values[0][1])}\")\n",
    "        \n",
    "        ############## np.sum does not work for heterogeneous size, so need to use good old loop addition \n",
    "        # self.mlp_all_layers_values = np.sum(self.mlp_all_layers_values,axis=0)\n",
    "        \n",
    "        # Average all samples of value per layer\n",
    "        # it's more like sum of all itterations, cause we haven't divided by total number of itteration count, those beong\n",
    "        # context_len * batch_number\n",
    "        self.average_per_layers_value = []\n",
    "        \n",
    "        for i in range(self.mlp.layers_num + 1):\n",
    "            # Creates zeros array of length equal to the specific layer value length\n",
    "            average_one_layer = np.zeros(len(self.mlp_all_layers_values[0][i]))\n",
    "            for j in range(len(self.mlp_all_layers_values)):\n",
    "                # here we loop through samples number so, j at start and i specifies the layer we are targeting\n",
    "                average_one_layer += self.mlp_all_layers_values[j][i]\n",
    "\n",
    "            average_one_layer /= self.context_len * self.training_batch_number\n",
    "            self.average_per_layers_value.append(average_one_layer)\n",
    "\n",
    "        # print(f'len of averaged up values array:::{len(self.average_per_layers_value)}\\n')\n",
    "        # print(f'len of all layers values:::::{len(self.mlp_all_layers_values)}')\n",
    "        # print(f'len of sel.err len:::::{len(self.err_per_batch_tokens)}')\n",
    "\n",
    "        # setting up error for layers other than normalization layer\n",
    "        self.mlp_error_val = np.sum(self.err_per_batch_tokens, axis=0)\n",
    "\n",
    "        print(f'the final mlp error value:::{self.mlp_error_val}')\n",
    "        \n",
    "        print(batch_predicted_token)\n",
    "        print(f'predicted_token_prob:{batch_predicted_token_prob}\\n')\n",
    "        print(f'Error_final::{self.err_per_batch_tokens}')\n",
    "        # print(batch_final_after_softmax)\n",
    "\n",
    "        self.backpropagation()\n",
    "        self.gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d12869fd-cddc-4a02-afb6-8b82f623b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create batches of training and validation data and also seperate test data, as we donot want the model to remeber, rather\n",
    "#learn patterns\n",
    "encode_ready_tokens = encode_text_assist(final_text)\n",
    "ready_tokens_len = len(encode_ready_tokens) \n",
    "train_text = encode_ready_tokens[:int(ready_tokens_len * 0.9)]\n",
    "validation_text = encode_ready_tokens[int(ready_tokens_len * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "6239e0d1-61de-4dad-ad92-81aa20d88b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the final mlp error value:::13849.006648468785\n",
      "[8624, 8624, 8624, 8624, 8624, 8624, 8624, 8624, 2323, 5529, 5414, 5414, 8624, 8624, 8624, 8624, 8624, 8624, 8624, 8624, 8624, 5529, 8624, 8624, 4559, 8624, 8624, 8624, 8624, 8624, 5414, 8624, 5414, 5529, 8624, 5529, 4559, 8624, 8624, 8624, 8624, 8624, 8624, 5414, 5414, 5414, 5414, 5414]\n",
      "predicted_token_prob:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Error_final::[-3.33333267e+06 -6.24999062e+05 -4.16665708e+05 ...  1.00000010e+00\n",
      "  1.00000010e+00  1.00000010e+00]\n",
      "gradients::::[array([ 4.99499761e+10,  2.69481061e+10,  4.64174648e+10,  2.48761000e+10,\n",
      "        3.81283951e+10, -6.26657665e+10,  1.47021325e+11,  1.17727477e+10,\n",
      "       -4.40464674e+10, -3.42771093e+10, -4.43181434e+10,  3.85161028e+09]), array([-2.56025143e+21, -2.37888601e+21, -2.69075287e+21, -2.23457751e+21,\n",
      "        1.46854986e+21, -4.56932914e+21, -1.39561202e+21, -1.26281411e+20,\n",
      "        3.01210167e+21, -1.57065442e+21,  2.09031688e+20,  1.93869632e+20]), array([-1.38126046e+21, -1.28341348e+21, -1.45166623e+21, -1.20555877e+21,\n",
      "        7.92285415e+20, -2.46516167e+21, -7.52935309e+20, -6.81290589e+19,\n",
      "        1.62503452e+21, -8.47371016e+20,  1.12772989e+20,  1.04593031e+20]), array([-2.37918794e+21, -2.21064886e+21, -2.50046019e+21, -2.07654600e+21,\n",
      "        1.36469258e+21, -4.24618171e+21, -1.29691297e+21, -1.17350666e+20,\n",
      "        2.79908289e+21, -1.45957620e+21,  1.94248762e+20,  1.80158982e+20]), array([-1.27505708e+21, -1.18473343e+21, -1.34004944e+21, -1.11286487e+21,\n",
      "        7.31367583e+20, -2.27561848e+21, -6.95043057e+20, -6.28907010e+19,\n",
      "        1.50008765e+21, -7.82217721e+20,  1.04102015e+20,  9.65510043e+19]), array([-1.95432082e+21, -1.81587886e+21, -2.05393669e+21, -1.70572362e+21,\n",
      "        1.12099052e+21, -3.48791332e+21, -1.06531475e+21, -9.63945915e+19,\n",
      "        2.29923238e+21, -1.19893015e+21,  1.59560493e+20,  1.47986816e+20]), array([ 3.21201592e+21,  2.98448021e+21,  3.37573917e+21,  2.80343503e+21,\n",
      "       -1.84239934e+21,  5.73254555e+21,  1.75089367e+21,  1.58428933e+20,\n",
      "       -3.77889390e+21,  1.97049671e+21, -2.62244989e+20, -2.43223122e+20]), array([-7.53577050e+21, -7.00194474e+21, -7.91988469e+21, -6.57719124e+21,\n",
      "        4.32248748e+21, -1.34492321e+22, -4.10780432e+21, -3.71693078e+20,\n",
      "        8.86573351e+21, -4.62301910e+21,  6.15257864e+20,  5.70630306e+20]), array([-6.03427599e+20, -5.60681446e+20, -6.34185582e+20, -5.26669266e+20,\n",
      "        3.46123630e+20, -1.07694865e+21, -3.28932854e+20, -2.97633615e+19,\n",
      "        7.09924524e+20, -3.70188731e+20,  4.92668368e+19,  4.56932805e+19]), array([ 2.25765936e+21,  2.09772923e+21,  2.37273704e+21,  1.97047633e+21,\n",
      "       -1.29498428e+21,  4.02928736e+21,  1.23066684e+21,  1.11356411e+20,\n",
      "       -2.65610613e+21,  1.38502126e+21, -1.84326563e+20, -1.70956487e+20]), array([ 1.75691812e+21,  1.63245996e+21,  1.84647196e+21,  1.53343132e+21,\n",
      "       -1.00776113e+21,  3.13560500e+21,  9.57709080e+20,  8.66579341e+19,\n",
      "       -2.06699074e+21,  1.07782821e+21, -1.43443552e+20, -1.33038914e+20]), array([ 2.27158447e+21,  2.11066791e+21,  2.38737195e+21,  1.98263011e+21,\n",
      "       -1.30297167e+21,  4.05413977e+21,  1.23825752e+21,  1.12043251e+20,\n",
      "       -2.67248884e+21,  1.39356399e+21, -1.85463479e+20, -1.72010937e+20]), array([-1.97419328e+20, -1.83434358e+20, -2.07482210e+20, -1.72306824e+20,\n",
      "        1.13238928e+20, -3.52338009e+20, -1.07614738e+20, -9.73747777e+18,\n",
      "        2.32261207e+20, -1.21112145e+20,  1.61182980e+19,  1.49491617e+19]), array([ 1.77101991e+30,  4.62712093e+30, -2.23220780e+29, -2.64590772e+30,\n",
      "       -3.82529972e+30, -8.81475001e+30, -6.82463742e+30, -6.13449099e+30,\n",
      "        2.17020853e+30, -3.27602034e+30, -5.62810550e+30, -1.02767844e+31]), array([ 1.64556279e+30,  4.29934074e+30, -2.07408064e+29, -2.45847451e+30,\n",
      "       -3.55431966e+30, -8.19032274e+30, -6.34118755e+30, -5.69993034e+30,\n",
      "        2.01647333e+30, -3.04395063e+30, -5.22941665e+30, -9.54878825e+30]), array([ 1.86129255e+30,  4.86297509e+30, -2.34598816e+29, -2.78077525e+30,\n",
      "       -4.02028335e+30, -9.26405650e+30, -7.17250365e+30, -6.44717899e+30,\n",
      "        2.28082866e+30, -3.44300604e+30, -5.91498196e+30, -1.08006139e+31]), array([ 1.54573930e+30,  4.03853319e+30, -1.94826231e+29, -2.30933799e+30,\n",
      "       -3.33870675e+30, -7.69347960e+30, -5.95651705e+30, -5.35415993e+30,\n",
      "        1.89414958e+30, -2.85929784e+30, -4.91218864e+30, -8.96953758e+30]), array([-1.01584986e+30, -2.65409785e+30,  1.28038537e+29,  1.51768197e+30,\n",
      "        2.19417645e+30,  5.05610496e+30,  3.91458442e+30,  3.51871922e+30,\n",
      "       -1.24482284e+30,  1.87911202e+30,  3.22825855e+30,  5.89472199e+30]), array([ 3.16077273e+30,  8.25811024e+30, -3.98386348e+29, -4.72220155e+30,\n",
      "       -6.82708476e+30, -1.57318511e+31, -1.21800595e+31, -1.09483421e+31,\n",
      "        3.87321221e+30, -5.84677546e+30, -1.00445863e+31, -1.83411716e+31]), array([ 9.65396071e+29,  2.52227789e+30, -1.21679301e+29, -1.44230390e+30,\n",
      "       -2.08519921e+30, -4.80498552e+30, -3.72016041e+30, -3.34395648e+30,\n",
      "        1.18299675e+30, -1.78578295e+30, -3.06792200e+30, -5.60195132e+30]), array([ 8.73534884e+28,  2.28227335e+29, -1.10101043e+28, -1.30506308e+29,\n",
      "       -1.88678441e+29, -4.34777248e+29, -3.36617269e+29, -3.02576603e+29,\n",
      "        1.07043001e+29, -1.61585877e+29, -2.77599730e+29, -5.06890389e+29]), array([-2.08358131e+30, -5.44374608e+30,  2.62616272e+29,  3.11287516e+30,\n",
      "        4.50041412e+30,  1.03704359e+31,  8.02909493e+30,  7.21714685e+30,\n",
      "       -2.55322140e+30,  3.85419425e+30,  6.62139108e+30,  1.20904999e+31]), array([ 1.08647933e+30,  2.83863057e+30, -1.36940732e+29, -1.62320256e+30,\n",
      "       -2.34673199e+30, -5.40764315e+30, -4.18675558e+30, -3.76336687e+30,\n",
      "        1.33137222e+30, -2.00976192e+30, -3.45271121e+30, -6.30456711e+30]), array([-1.44594893e+29, -3.77781216e+29,  1.82248571e+28,  2.16025095e+29,\n",
      "        3.12316536e+29,  7.19680127e+29,  5.57197415e+29,  5.00850421e+29,\n",
      "       -1.77186642e+29,  2.67470629e+29,  4.59506586e+29,  8.39047905e+29]), array([-1.34106742e+29, -3.50378960e+29,  1.69029222e+28,  2.00355774e+29,\n",
      "        2.89662742e+29,  6.67478328e+29,  5.16781255e+29,  4.64521374e+29,\n",
      "       -1.64334458e+29,  2.48069721e+29,  4.26176403e+29,  7.78187798e+29]), array([-3.44582535e+29,  8.37990698e+28, -4.56380169e+28,  2.12566306e+27,\n",
      "       -1.11877855e+29, -1.97977871e+29, -5.60005990e+28, -1.38484537e+29,\n",
      "       -8.52452090e+27, -1.07146353e+28, -9.52515551e+28,  2.30898771e+29]), array([-3.20172684e+29,  7.78628351e+28, -4.24050695e+28,  1.97508341e+27,\n",
      "       -1.03952550e+29, -1.83953334e+29, -5.20335777e+28, -1.28674444e+29,\n",
      "       -7.92065315e+27, -9.95562227e+27, -8.85040388e+28,  2.14542154e+29]), array([-3.62146637e+29,  8.80704859e+28, -4.79642833e+28,  2.23401261e+27,\n",
      "       -1.17580507e+29, -2.08069222e+29, -5.88550682e+28, -1.45543387e+29,\n",
      "       -8.95903380e+27, -1.12607830e+28, -1.00106729e+29,  2.42668171e+29]), array([-3.00750299e+29,  7.31395029e+28, -3.98326840e+28,  1.85527047e+27,\n",
      "       -9.76465579e+28, -1.72794317e+29, -4.88771055e+28, -1.20868766e+29,\n",
      "       -7.44016876e+27, -9.35169214e+27, -8.31351876e+28,  2.01527551e+29]), array([ 1.97651148e+29, -4.80668075e+28,  2.61777819e+28, -1.21927173e+27,\n",
      "        6.41726853e+28,  1.13559306e+29,  3.21217171e+28,  7.94341703e+28,\n",
      "        4.88963071e+27,  6.14587150e+27,  5.46359067e+28, -1.32442601e+29]), array([-6.14982967e+29,  1.49557785e+29, -8.14510318e+28,  3.79371106e+27,\n",
      "       -1.99670524e+29, -3.53334850e+29, -9.99453282e+28, -2.47155972e+29,\n",
      "       -1.52138737e+28, -1.91226123e+28, -1.69997252e+29,  4.12089404e+29]), array([-1.87834492e+29,  4.56794936e+28, -2.48776210e+28,  1.15871468e+27,\n",
      "       -6.09854476e+28, -1.07919204e+29, -3.05263413e+28, -7.54889469e+28,\n",
      "       -4.64677949e+27, -5.84062709e+27, -5.19223282e+28,  1.25864630e+29]), array([-1.69961310e+28,  4.13329123e+27, -2.25104187e+27,  1.04845847e+26,\n",
      "       -5.51824453e+27, -9.76502721e+27, -2.76216413e+27, -6.83058803e+27,\n",
      "       -4.20462037e+26, -5.28486873e+26, -4.69817169e+27,  1.13888122e+28]), array([ 4.05396758e+29, -9.85884882e+28,  5.36925183e+28, -2.50081425e+27,\n",
      "        1.31622805e+29,  2.32918325e+29,  6.58839581e+28,  1.62925211e+29,\n",
      "        1.00289852e+28,  1.26056256e+28,  1.12062184e+29, -2.71649325e+29]), array([-2.11393333e+29,  5.14087711e+28, -2.79978569e+28,  1.30404461e+27,\n",
      "       -6.86344498e+28, -1.21454797e+29, -3.43550589e+28, -8.49570273e+28,\n",
      "       -5.22959437e+27, -6.57317840e+27, -5.84346032e+28,  1.41651000e+29]), array([ 2.81334357e+28, -6.84177374e+27,  3.72611517e+27, -1.73549727e+26,\n",
      "        9.13426574e+27,  1.61639001e+28,  4.57216803e+27,  1.13065679e+28,\n",
      "        6.95984376e+26,  8.74796235e+26,  7.77681172e+27, -1.88517266e+28]), array([ 2.60927847e+28, -6.34550758e+27,  3.45584244e+27, -1.60961346e+26,\n",
      "        8.47171431e+27,  1.49914561e+28,  4.24052709e+27,  1.04864491e+28,\n",
      "        6.45501342e+26,  8.11343133e+26,  7.21272284e+27, -1.74843218e+28]), array([-7.56567890e+28, -2.82308245e+28,  1.02485785e+28,  1.47815677e+28,\n",
      "       -1.48494771e+28, -4.27549429e+28, -3.94339380e+27, -6.64489270e+28,\n",
      "       -2.08294648e+28, -1.19826003e+28,  4.14005320e+28,  3.03333576e+28]), array([-7.02973446e+28, -2.62309837e+28,  9.52258038e+27,  1.37344576e+28,\n",
      "       -1.37975564e+28, -3.97262294e+28, -3.66404809e+27, -6.17417576e+28,\n",
      "       -1.93539283e+28, -1.11337660e+28,  3.84677635e+28,  2.81845757e+28]), array([-7.95131759e+28, -2.96698095e+28,  1.07709703e+28,  1.55350155e+28,\n",
      "       -1.56063864e+28, -4.49342530e+28, -4.14439694e+27, -6.98359696e+28,\n",
      "       -2.18911868e+28, -1.25933788e+28,  4.35108048e+28,  3.18795131e+28]), array([-6.60329518e+28, -2.46397541e+28,  8.94491954e+27,  1.29012949e+28,\n",
      "       -1.29605660e+28, -3.73163483e+28, -3.44177881e+27, -5.79963656e+28,\n",
      "       -1.81798761e+28, -1.04583670e+28,  3.61342237e+28,  2.64748368e+28]), array([ 4.33964282e+28,  1.61930868e+28, -5.87854318e+27, -8.47864743e+27,\n",
      "        8.51760003e+27,  2.45240623e+28,  2.26191474e+27,  3.81148358e+28,\n",
      "        1.19476968e+28,  6.87317106e+27, -2.37471777e+28, -1.73990912e+28]), array([-1.35026102e+29, -5.03840864e+28,  1.82908319e+28,  2.63809434e+28,\n",
      "       -2.65021427e+28, -7.63055553e+28, -7.03784950e+27, -1.18592657e+29,\n",
      "       -3.71747398e+28, -2.13855734e+28,  7.38883125e+28,  5.41365170e+28]), array([-4.12410761e+28, -1.53888316e+28,  5.58657605e+27,  8.05754203e+27,\n",
      "       -8.09455998e+27, -2.33060361e+28, -2.14957317e+27, -3.62218023e+28,\n",
      "       -1.13542956e+28, -6.53180417e+27,  2.25677367e+28,  1.65349379e+28]), array([-3.73168275e+27, -1.39245244e+27,  5.05499163e+26,  7.29083560e+26,\n",
      "       -7.32433115e+26, -2.10883762e+27, -1.94503293e+26, -3.27751571e+27,\n",
      "       -1.02738902e+27, -5.91027763e+26,  2.04203288e+27,  1.49615743e+27]), array([ 8.90092035e+28,  3.32131886e+28, -1.20573159e+28, -1.73903173e+28,\n",
      "        1.74702118e+28,  5.03006203e+28,  4.63935023e+27,  7.81762767e+28,\n",
      "        2.45055876e+28,  1.40973695e+28, -4.87071739e+28, -3.56867908e+28]), array([-4.64136721e+28, -1.73189511e+28,  6.28726342e+27,  9.06814635e+27,\n",
      "       -9.10980722e+27, -2.62291584e+28, -2.41917995e+27, -4.07648640e+28,\n",
      "       -1.27783899e+28, -7.35104527e+27,  2.53982589e+28,  1.86088061e+28]), array([ 6.17699736e+27,  2.30490523e+27, -8.36745031e+26, -1.20684086e+27,\n",
      "        1.21238533e+27,  3.49072665e+27,  3.21958325e+26,  5.42522161e+27,\n",
      "        1.70062133e+27,  9.78319213e+26, -3.38014579e+27, -2.47656651e+27]), array([ 5.72895057e+27,  2.13771957e+27, -7.76051962e+26, -1.11930299e+27,\n",
      "        1.12444529e+27,  3.23752776e+27,  2.98605168e+26,  5.03170466e+27,\n",
      "        1.57726723e+27,  9.07357101e+26, -3.13496785e+27, -2.29692944e+27]), array([ 9.21951904e+26, -1.46622708e+27,  2.63597675e+26, -4.57107502e+26,\n",
      "       -4.45195093e+26,  2.88984256e+26, -5.70802688e+26,  6.79348528e+26,\n",
      "        6.00881945e+26,  2.18175999e+27,  1.83012848e+27,  9.60560488e+26]), array([ 8.56641837e+26, -1.36236115e+27,  2.44924703e+26, -4.24726505e+26,\n",
      "       -4.13657958e+26,  2.68512926e+26, -5.30367648e+26,  6.31224219e+26,\n",
      "        5.58316124e+26,  2.02720649e+27,  1.70048418e+27,  8.92515431e+26]), array([ 9.68945746e+26, -1.54096378e+27,  2.77033807e+26, -4.80407240e+26,\n",
      "       -4.67887630e+26,  3.03714396e+26, -5.99897710e+26,  7.13976362e+26,\n",
      "        6.31510171e+26,  2.29296892e+27,  1.92341401e+27,  1.00952229e+27]), array([ 8.04676044e+26, -1.27971730e+27,  2.30067027e+26, -3.98961654e+26,\n",
      "       -3.88564549e+26,  2.52224337e+26, -4.98194371e+26,  5.92932758e+26,\n",
      "        5.24447430e+26,  1.90423165e+27,  1.59732904e+27,  8.38373466e+26]), array([-5.28827883e+26,  8.41021920e+26, -1.51198560e+26,  2.62195014e+26,\n",
      "        2.55362104e+26, -1.65760200e+26,  3.27410113e+26, -3.89671568e+26,\n",
      "       -3.44663454e+26, -1.25144871e+27, -1.04975430e+27, -5.50973611e+26]), array([ 1.64542500e+27, -2.61680319e+27,  4.70447755e+26, -8.15808404e+26,\n",
      "       -7.94548101e+26,  5.15755667e+26, -1.01872235e+27,  1.21244617e+27,\n",
      "        1.07240537e+27,  3.89382832e+27,  3.26626492e+27,  1.71433047e+27]), array([ 5.02562812e+26, -7.99251239e+26,  1.43689045e+26, -2.49172685e+26,\n",
      "       -2.42679142e+26,  1.57527458e+26, -3.11148773e+26,  3.70317914e+26,\n",
      "        3.27545199e+26,  1.18929353e+27,  9.97616593e+26,  5.23608637e+26]), array([ 4.54742008e+25, -7.23199378e+25,  1.30016474e+25, -2.25462936e+25,\n",
      "       -2.19587279e+25,  1.42538109e+25, -2.81541759e+25,  3.35080726e+25,\n",
      "        2.96378001e+25,  1.07612763e+26,  9.02689498e+25,  4.73785241e+25]), array([-1.08466412e+27,  1.72499660e+27, -3.10119149e+26,  5.37780881e+26,\n",
      "        5.23766090e+26, -3.39986124e+26,  6.71541751e+26, -7.99244485e+26,\n",
      "       -7.06929597e+26, -2.56681154e+27, -2.15312176e+27, -1.13008661e+27]), array([ 5.65595949e+26, -8.99496048e+26,  1.61711014e+26, -2.80424771e+26,\n",
      "       -2.73116786e+26,  1.77285088e+26, -3.50174110e+26,  4.16764447e+26,\n",
      "        3.68627032e+26,  1.33845877e+27,  1.12274106e+27,  5.89281413e+26]), array([-7.52727488e+25,  1.19710087e+26, -2.15214280e+25,  3.73205349e+25,\n",
      "        3.63479464e+25, -2.35941150e+25,  4.66031765e+25, -5.54653999e+25,\n",
      "       -4.90589970e+25, -1.78129760e+26, -1.49420811e+26, -7.84249460e+25]), array([-6.98128608e+25,  1.11026949e+26, -1.99603772e+25,  3.46135002e+25,\n",
      "        3.37114582e+25, -2.18827224e+25,  4.32228280e+25, -5.14422325e+25,\n",
      "       -4.55005163e+25, -1.65209168e+26, -1.38582614e+26, -7.27364143e+25]), array([-3.01431449e+26, -3.83741328e+26]), array([-2.80078374e+26, -3.56557511e+26]), array([-3.16796049e+26, -4.03301437e+26]), array([-2.63088199e+26, -3.34927942e+26]), array([1.72899860e+26, 2.20112474e+26]), array([-5.37970408e+26, -6.84870406e+26]), array([-1.64312516e+26, -2.09180240e+26]), array([-1.48677541e+25, -1.89275928e+25]), array([3.54630081e+26, 4.51466556e+26]), array([-1.84921150e+26, -2.35416337e+26]), array([2.46103659e+25, 3.13305547e+25]), array([2.28252599e+25, 2.90580016e+25])]\n",
      "length of gradients::73\n"
     ]
    }
   ],
   "source": [
    "# print(encode_ready_tokens[:100])\n",
    "batch_size = 4\n",
    "def create_batches(context_len,number_of_batches):\n",
    "    #8 being context_len and we actually randint(0,len(final_text) - context_len + 1) if randint were inclusive but it's not so fine with,\n",
    "    # - context_len+1 as for eg randint(0,8) will give value up to 7 \n",
    "    # np.random.randint(0,len(final_text) - 8, size=4)\n",
    "    random_pick = np.random.randint(0,len(train_text) - context_len,size=number_of_batches)\n",
    "    train_data_batch = [encode_list(train_text[i:i+context_len]) for i in random_pick]\n",
    "    test_data_batch = [encode_list(train_text[i+1:i+context_len+1]) for i in random_pick]\n",
    "    \n",
    "    #return training_data, and each subsequent output as expected prediction,eg:\n",
    "    # train_data_batch = [[1,2,3,4,5],[---]], and xb = [[2,3,4,5,6],[---]], so here context_len is 5 and each index in xa would have it's expected \n",
    "    #output in xb\n",
    "    return train_data_batch, test_data_batch\n",
    "\n",
    "# This data kinda suck, while having a shorter context-len like 8, so, increased to 12\n",
    "model = Model(12,4)\n",
    "\n",
    "for _ in range(1):\n",
    "    train_data, expected_prediction = create_batches(model.context_len,model.training_batch_number)\n",
    "    model.train(train_data,expected_prediction,lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d67da0a6-a617-4519-abbf-89c94122e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  4]\n",
      "  [ 1  6]]\n",
      "\n",
      " [[ 2  6]\n",
      "  [ 2  9]]\n",
      "\n",
      " [[ 3  8]\n",
      "  [ 3 12]]]\n",
      "[[[ 1  1]\n",
      "  [ 4  6]]\n",
      "\n",
      " [[ 2  2]\n",
      "  [ 6  9]]\n",
      "\n",
      " [[ 3  3]\n",
      "  [ 8 12]]]\n",
      "[0.65900114 0.24243297 0.09856589]\n",
      "[[1], [2], [3], [4], [5]]\n",
      "[2 3 4 5 6]\n",
      "log_softmax::[    0.  -999. -2000.]\n",
      "-100000000.0\n",
      "[[3 6]\n",
      " [4 6]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "B = np.array([[1, 2], [1, 3]])\n",
    "\n",
    "result = A[:, np.newaxis, :] * B  # Broadcasting\n",
    "print(result)\n",
    "print(result.transpose(0,2,1))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max(x) for numerical stability\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Example usage:\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "softmax_values = softmax(x)\n",
    "print(softmax_values)\n",
    "\n",
    "\n",
    "a = np.array([1, 2, 3, 4])  # Shape: (4,)\n",
    "b = np.array([[1, 2, 3, 4],  # Shape: (4, N)\n",
    "              [1, 2, 3, 4],  \n",
    "              [1, 2, 3, 4],  \n",
    "              [1, 2, 3, 4]])\n",
    "\n",
    "result = b * a[:, np.newaxis]  # Expanding `a` to (4, 1) and broadcasting\n",
    "# print(result)\n",
    "\n",
    "# print(b * np.array([[1],[2],[3],[4]])) # same as what we did before with np.newaxis\n",
    "\n",
    "print([[1],[2],[3]] + [[4],[5]])\n",
    "\n",
    "a = np.array([[1.13645561e-02, 1.52903492e-02],\n",
    " [4.53067962e+01, 6.55301006e+01],\n",
    " [6.57354624e-03, 1.17096469e-01]])\n",
    "np.sum(a,axis=1)\n",
    "print(np.array([1,2,3,4,5]) + np.array([abs(-1)]))\n",
    "\n",
    "x = np.array([1000, 1001, 1002])\n",
    "x_max = np.max(x, axis=-1, keepdims=True)  # Find max value (1002)\n",
    "x_stable = x - x_max  # Subtract 1002 from all elements  [-2, -1, 0]\n",
    "\n",
    "exp_x = np.exp(x_stable)  # Now, exp(x_stable) = [e^(-2), e^(-1), e^0]\n",
    "# softmax = exp_x / np.sum(exp_x)\n",
    "# print(softmax)  # Outputs stable probabilities\n",
    "\n",
    "def log_softmax(x, dtype=np.float32):\n",
    "    x = np.asarray(x, dtype=dtype)\n",
    "    x_max = np.max(x, axis=-1, keepdims=True)\n",
    "    log_sum_exp = np.log(np.sum(np.exp(x - x_max), axis=-1, keepdims=True))\n",
    "    return x - x_max - log_sum_exp\n",
    "\n",
    "print(f'log_softmax::{log_softmax(np.array([1000, 1, -1000]))}')\n",
    "\n",
    "def negative_loss_loss_penalty(ground_truth,predicted_value):\n",
    "        lowest_possible_value = 0.00000001\n",
    "        clipped_val = np.clip(predicted_value, lowest_possible_value, 1-lowest_possible_value)\n",
    "        return (clipped_val - ground_truth)/(clipped_val * (1 - clipped_val))\n",
    "\n",
    "print(negative_loss_loss_penalty(1,0))\n",
    "\n",
    "\n",
    "arr = np.array([[[1,2],[2,2],[3,2]], \n",
    "                [[1,2],[1,2],[1,2]], \n",
    "                [[1,2],[1,2],[1,2]]])\n",
    "\n",
    "result = np.sum(arr, axis=0)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0a05a-730e-4b26-9d4a-63382bb1bb7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
