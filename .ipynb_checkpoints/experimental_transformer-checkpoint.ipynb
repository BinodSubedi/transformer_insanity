{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "febc066d-604c-4486-929c-4467b7515366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "b2095493-fa37-4bd0-b4b0-deb0b2b5eb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "print(text[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6c58a107-6856-4e81-b384-6da35dc8c5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ',', '.', '\\n', '?', '!', \"'\", ':', ';', '--', '3', 'A', 'ABHORSON', 'ABRAHAM', 'ADRIAN', 'AEacides', 'AEdile', 'AEdiles', 'AEneas', 'AEsop', 'ALL', 'ALONSO', 'ANGELO', 'ANNE', 'ANOTHER', 'ANTIGONUS', 'ANTONIO', 'ARCHBISHOP', 'ARCHIDAMUS', 'ARIEL', 'AUFIDIUS', 'AUMERLE', 'AUTOLYCUS', 'Abase', 'Abate', 'Abated', 'Abbot', 'Abel', 'Abhorred', 'Abhorson', 'Abides', 'Able', 'About', 'Above', 'Abraham', 'Absolute', 'Accept', 'Accomplish', 'According', 'Accords', 'Account', 'Accountant', 'Accursed', 'Accuse', 'Achieve', 'Acquaint', 'Action', 'Adam', 'Add', 'Added', 'Adding', 'Address', 'Adieu', 'Adjudged', 'Admit', 'Adonis', 'Adoptedly', 'Adopts', 'Adrian', 'Adriatic', 'Advance', 'Advantaging', 'Adversity', 'Advertising', 'Advocate', 'Affection', 'Affliction', 'Affrighted', 'Affrights', 'Affront', 'Afore', 'Afresh', 'Afric', 'African', 'After', 'Again', 'Against', 'Agamemnon', 'Age', 'Aged', 'Agenor', 'Agreed', 'Agrippa', 'Ah', 'Aim', 'Aiming', 'Airy', 'Ajax', 'Al', 'Alack']\n"
     ]
    }
   ],
   "source": [
    "def text_split_assist(text):\n",
    "    comma_splitted = ' '.join(text.split(','))\n",
    "    newLine_split = ' '.join(comma_splitted.split('\\n'))\n",
    "    # remaining are removing &c, :, --, ', ?, !, ., ;\n",
    "    colon_split = ' '.join(newLine_split.split(\":\"))\n",
    "    double_dash_split = ' '.join(colon_split.split('--'))\n",
    "    kotation_split = ' '.join(double_dash_split.split(\"'\"))\n",
    "    question_mark_split = ' '.join(kotation_split.split('?'))\n",
    "    exclamation_split = ' '.join(question_mark_split.split('!'))\n",
    "    fullstop_split = ' '.join(exclamation_split.split('.'))\n",
    "    semiColon_split = ' '.join(fullstop_split.split(';'))\n",
    "    garbage_removable_split = ' '.join(semiColon_split.split('&C'))\n",
    "    final_split = ' '.join(garbage_removable_split.split('&c'))\n",
    "    return final_split\n",
    "\n",
    "final_split = text_split_assist(text)\n",
    "\n",
    "total_tokens = sorted(set(list(final_split.split(' '))))\n",
    "# Adding all the unique necessary elements we removed\n",
    "unique_elements = [' ',',','.','\\n','?','!', \"'\",':',';','--']\n",
    "# print(total_tokens[len(total_tokens) - 1])\n",
    "# to remove '' token\n",
    "total_tokens.pop(0)\n",
    "total_tokens = unique_elements + total_tokens\n",
    "print(total_tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "b4bc1c51-07fa-4c80-a539-6c4569dd6cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13851\n",
      "[22, 8, 0, 9]\n",
      "ANGELO; --\n"
     ]
    }
   ],
   "source": [
    "#very simple tokenizer\n",
    "clean_text1 = ' '.join(text.split('&C'))\n",
    "final_text = ' '.join(clean_text1.split('&c'))\n",
    "# print(final_text[:100])\n",
    "\n",
    "import re\n",
    "\n",
    "def encode_text_assist(plain_text):\n",
    "    pattern = r\"(--|[,.\\n?!':;\\s])\"\n",
    "    plain_text = re.split(pattern, plain_text)\n",
    "    #joining with spaces\n",
    "    # plain_text = ' '.join(plain_text)\n",
    "    #now splitting through spaces\n",
    "    # plain_text = plain_text.split(' ')\n",
    "    tokens_arr = []\n",
    "    for i,t in enumerate(plain_text):\n",
    "        # print(f't:{t}')\n",
    "        # t = t.strip()\n",
    "        if t and t in total_tokens:\n",
    "            tokens_arr.append(t)\n",
    "            continue\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return tokens_arr\n",
    "\n",
    "vocab_size = len(total_tokens)\n",
    "word_to_int = {w:i for i,w in enumerate(total_tokens)}\n",
    "int_to_word = {i:w for i,w in enumerate(total_tokens)}\n",
    "# in encode I need to create some function to split up words\n",
    "encode = lambda l: [word_to_int[w] for w in encode_text_assist(l)]\n",
    "encode_list = lambda l: [word_to_int[w] for w in l]\n",
    "decode = lambda l: ''.join([int_to_word[i] for i in l])\n",
    "print(vocab_size)\n",
    "print(encode(\"ANGELO; --\"))\n",
    "print(decode(encode(\"ANGELO; --\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "00b85913-35e1-48c1-b356-4ed419f1b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Perceptron/neuron Node, Layer and MLP Class\n",
    "# class Neuron:\n",
    "#     def __init__(self,prev_layer,forward_layer,activation,embedding_per_vocab):\n",
    "#         # self.prev_layer = prev_layer\n",
    "#         # self.forward_layer = forward_layer \n",
    "#         self.activation = activation\n",
    "#         self.initial_random_w_gen()\n",
    "#         self.embedding_per_vocab = embedding_per_vocab\n",
    "        \n",
    "        \n",
    "\n",
    "#     # random weight generation for initial setup\n",
    "#     def initial_random_w_gen(self):\n",
    "#         self.weight = [self.random_param_gen() for _ in range(self.embedding_per_vocab)]\n",
    "\n",
    "#     # Instead of creating bias per neuron, we will create biases arrays per layer, which just makes life easier working with matrix operations\n",
    "#     # adding biases to the final mat mul of weights and input values\n",
    "#     # def initial_bias_gen(self):\n",
    "#     #     self.bias = self.random_param_gen()\n",
    "        \n",
    "#     def random_param_gen(self,range = 8):\n",
    "#         return np.random.uniform(-range,range)\n",
    "\n",
    "#     def apply_relu(self,val):\n",
    "#         # relu simply means, if negative or zero, the value is 0 and if positive, y = x\n",
    "#         np.maximum(0,val)\n",
    "\n",
    "#     def forward_pass(self):\n",
    "#         pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "df84f317-20dc-4b0f-a7dc-a14fcf8ad603",
   "metadata": {},
   "outputs": [],
   "source": [
    "#putting softmax on global scope, as it is used in many clases\n",
    "# def temperature_scaled_softmax(logits, T=1.0):\n",
    "#     logits = np.array(logits)  # Ensure it's a NumPy array\n",
    "#     scaled_logits = logits / T  # Apply temperature scaling\n",
    "#     exp_values = np.exp(scaled_logits - np.max(scaled_logits))  # Stability trick\n",
    "#     return exp_values / np.sum(exp_values)\n",
    "\n",
    "def softmax(x, dtype=np.float32):\n",
    "    # \"\"\"Numerically stable softmax with float32 or float64 support.\"\"\"\n",
    "    x = np.asarray(x, dtype=dtype)  # Ensure correct dtype\n",
    "    x_max = np.max(x, axis=-1, keepdims=True)  # Stability trick\n",
    "    exp_x = np.exp(x - x_max, dtype=dtype)  # Compute exp with correct dtype\n",
    "    # print(f'softmax_valllll:{np.sum(exp_x, axis=-1, keepdims=True, dtype=dtype)}')\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True, dtype=dtype)\n",
    "\n",
    "    # I need to use adaptive scaling strategy, where I initially use average probaility measuring technique and when the output variation increases more\n",
    "    # I will opt to using real softmax\n",
    "    #Normalizing negative values to 0 by adding smallest value positive alternate to zero down\n",
    "    \n",
    "    # smallest_element = abs(min(x)) # abs to convert to absoulte positive value\n",
    "    # equalizing_negative_val = x + np.array([smallest_element])\n",
    "    # sum_val = np.sum(equalizing_negative_val, axis=0)\n",
    "    # normalized_val = np.array(equalizing_negative_val / sum_val, dtype=dtype)\n",
    "    # return normalized_val\n",
    "\n",
    "    # return temperature_scaled_softmax(x,100)\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self, nodes_num,embedding_per_vocab=None, bias = True,prev_layer = None, forward_layer = None,activation = 'relu',type='hidden'):\n",
    "        self.nodes_num = nodes_num\n",
    "        self.bias = bias\n",
    "        self.activation = activation\n",
    "        self.prev_layer = prev_layer\n",
    "        self.forward_layer = forward_layer\n",
    "        self.type = type\n",
    "        # self.bias = bias\n",
    "        self.embedding_per_vocab = embedding_per_vocab\n",
    "        # We could choose some layer without any bias\n",
    "        if bias:\n",
    "            self.initial_bias_gen(range=2)\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.create()\n",
    "\n",
    "    def initial_bias_gen(self,range):\n",
    "        self.biases = self.__gen_random_numbers(self.nodes_num,range)\n",
    "    \n",
    "    def __gen_random_numbers(self,size,range):\n",
    "        #not the best way, but it works starting out with any random number\n",
    "        # return range * np.random.rand(size)\n",
    "        # so, we use something better like:\n",
    "        return np.random.uniform(-range,range,size=size)\n",
    "\n",
    "    def apply_relu(self,val):\n",
    "        # relu simply means, if negative or zero, the value is 0 and if positive, y = x\n",
    "        np.maximum(0,val)\n",
    "    \n",
    "    def create(self):\n",
    "        # Instead of explicitly Making another Neuron class and creating segmentation\n",
    "        # I would use the better option to create matrix of weights per node per layer, and also, simd is just better usable this way\n",
    "        # self.nodes = [Neuron(self.prev_layer,self.forward_layer,self.activation,embedding_per_vocab=self.embedding_per_vocab) for _ in range(self.nodes_num)]\n",
    "        self.nodes = [self.__gen_random_numbers(self.embedding_per_vocab,range=8) for _ in range(self.nodes_num)] if self.type == 'input' else [\n",
    "            self.__gen_random_numbers(self.prev_layer.nodes_num,range=8) for _ in range(self.nodes_num)]\n",
    "\n",
    "    # Receive the data processed by previuos MLP layer or if the layer is first, receives the output from mskd-self-attention layer\n",
    "    def forward_pass(self,input_arr):\n",
    "        element_wise_weight_mmul = np.array(self.nodes) * np.array([input_arr])\n",
    "        mmul_val_addition = np.sum(element_wise_weight_mmul, axis=1)\n",
    "        if(self.bias == None):\n",
    "            return mmul_val_addition\n",
    "        else:\n",
    "            bias_addition = np.array(mmul_val_addition) + np.array(self.biases)\n",
    "            # return processed vale with weights and input arr\n",
    "            return bias_addition\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ccaab3c7-ad2a-4099-af9c-52908d32acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, layers_num, nodes_per_layer,embedding_per_vocab,custom_layer_config={}):\n",
    "        self.layers_num = layers_num\n",
    "        self.nodes_per_layer = nodes_per_layer\n",
    "        self.embedding_per_vocab = embedding_per_vocab\n",
    "        self.mlp_allLayers_values = []\n",
    "        self.initiate()\n",
    "\n",
    "    # def initiate(self):\n",
    "    #     self.head = Layer(self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab)\n",
    "    #     current_layer = self.head\n",
    "    #     for _ in range(self.layers_num - 1):\n",
    "    #         new_layer = Layer(nodes_num=self.nodes_per_layer)\n",
    "    #         current_layer.forward_layer = new_layer\n",
    "    #         new_layer.prev_layer = current_layer\n",
    "    #         current_layer = new_layer\n",
    "\n",
    "    # In future we are going to create matrix for weights of each layer and each neuron inside it, which will be better approach then using linked-lst\n",
    "    # style linking layers.\n",
    "    def initiate(self):\n",
    "        #this is the array implementation of layers and neurons mapping, which is better than using above lnkd-lst method\n",
    "        # we did if else here because in the layer class, we need to be able to distinguish between layer types and position\n",
    "        # to assign specific weights shape\n",
    "        # self.layers = [Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,type='input') if i==0 else\n",
    "        #                Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,prev_layer=self.layers[i-1])\n",
    "        #                for i in range(self.layers_num)]\n",
    "\n",
    "        # self.layers = [Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,prev_layer=self.layers[i-1])\n",
    "        #                for _ in range(self.layers_num)]\n",
    "        \n",
    "        # self.layers[0] = Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,type='input')\n",
    "\n",
    "        # Neither of above way worked as, self.layers[i - 1] couldn't be assigned sooo...\n",
    "        self.layers = []\n",
    "        for i in range(self.layers_num):\n",
    "            if i == 0:\n",
    "                self.layers.append(Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,type='input'))\n",
    "            else:\n",
    "                self.layers.append(Layer(nodes_num=self.nodes_per_layer,embedding_per_vocab=self.embedding_per_vocab,prev_layer=self.layers[i-1]))\n",
    "        \n",
    "    # here we will attach the self attention masking layer\n",
    "    ## Or not, cause we could just use model class to indirectly create propagation between MLP and self attention layers or any\n",
    "    # def attach_to_head(self, attention_layer):\n",
    "            \n",
    "    def feed_forward(self,masked_self_attn_embedd_arr):\n",
    "\n",
    "        # Removing any data that existed before\n",
    "        # I opted to keep data according to data processed in matrix form\n",
    "        # making easier for final averaging gradient on position basis\n",
    "        self.mlp_allLayers_values = []\n",
    "        \n",
    "        #Now element wise matrix mul with weights of neurons in layer and mskd_self_attn_embedd_arr val, one layer at a time, and\n",
    "        # also adding bias at the end of neuron wise mat mul\n",
    "        data_to_process = masked_self_attn_embedd_arr\n",
    "        for layer in self.layers:\n",
    "            # this add all the values output from slef-attn layer output adjacent input to second-last layer, as at last layer the loop ends\n",
    "            self.mlp_allLayers_values.append(data_to_process)\n",
    "            data_to_process = layer.forward_pass(data_to_process)\n",
    "\n",
    "        # Adding last layer or the layer before normalization layer\n",
    "        self.mlp_allLayers_values.append(data_to_process)\n",
    "        # print(f'length of values assigned to corresponding layer::{len(self.mlp_allLayers_values)}')\n",
    "        # returns the arr after getting operated through all MLP layers\n",
    "        # now Model class will pass this return value to normalizer for getting probability distribution out of it\n",
    "        return data_to_process, self.mlp_allLayers_values\n",
    "        \n",
    "        \n",
    "# layer = Layer(10)\n",
    "# for n in layer.nodes:\n",
    "#     print(f'n.weight: {n.weight}')\n",
    "#     print(f'n.bias: {n.bias}\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "067b4c62-60bc-4898-b066-32abbe3c06c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedSelfAttLayer:\n",
    "    def __init__(self,context_len=8,initial_base_f =20,positional_function_f_change = 1.5,embedding_len_per_vocab = 2):\n",
    "        self.context_len = context_len\n",
    "        # it will look like: [ [[query final value],[key final value],[value final value],[embedding at the last, that is after adding self-attn value]],\n",
    "        #[---same as before---], [---same as before---] ]\n",
    "        self.context_history = {}\n",
    "        # this means, for vector, each index gets uplift of 30, like 30, 60, 90, as f increases like x2, it will increase to 60, 120, 180 and so on\n",
    "        self.base_frequency = initial_base_f \n",
    "        self.frequency_change_rate = positional_function_f_change\n",
    "        self.embedding_per_vocab = embedding_len_per_vocab\n",
    "        self.gen_first_weights()\n",
    "        self.gen_initial_query_key_value_w()\n",
    "    \n",
    "    #start input propagation\n",
    "    def start_input_prop(self,data,mode='train'):\n",
    "        # In train mode, we need to train the layers for guessing every next token,\n",
    "        # but in trained model, we only use other token to get masked self-attention value for very last token, and then run through MLP\n",
    "        inference_len = len(data)\n",
    "        # we say inference_len because we will always get complete 8 tokens for training\n",
    "        data_np = np.array([data])\n",
    "        # eg [[1,2],[3,4]] * [3,4] = [[3,8],[9,16]]\n",
    "        initial_values = np.array(self.first_weights) * data_np\n",
    "        # Adding positional encodings, sine cosine alternating function values\n",
    "        self.setup_main_self_attn_work(initial_values)\n",
    "        \n",
    "    def setup_main_self_attn_work(self,init_values):\n",
    "        # Here we itteratively create context_len values and then try to train the MLP layer # I am still tinkering so, might not be correct\n",
    "        for i in range(self.context_len):\n",
    "            #setting up positional encoding(means adding pos. encoding for every context_len vals)\n",
    "            for j in range (self.embedding_per_vocab):\n",
    "                # so essentially i is token number and j is the embedding number\n",
    "                # we did i+1 and j+1 cause, the alternating function takes position that starts with 1 not 0 like indexing in code\n",
    "                init_values[i][j] += self.alternating_sine_cosine_p_value(i+1, j+1)\n",
    "        # after adding positional_values\n",
    "        self.after_positional_values = init_values\n",
    "        # now multiplying to get query, key and values parameter context wise value\n",
    "        self.context_history['query'] = self.after_positional_values * np.array(self.query_weights)\n",
    "        self.context_history['key'] = self.after_positional_values * np.array(self.key_weights)\n",
    "        self.context_history['value'] = self.after_positional_values * np.array(self.value_weights)\n",
    "            \n",
    "\n",
    "    #generate initial weight for the very first layer of weights for inputs, we are only using two size vector for data representation\n",
    "    #with weights for each context_len assigned value, means each vector contains context_len(default 8) weights\n",
    "    def gen_first_weights(self):\n",
    "        self.first_weights = [self.__gen_random_numbers(self.context_len) for _ in range(self.embedding_per_vocab)]\n",
    "        \n",
    "        \n",
    "    #generate initial random query, key and value weights\n",
    "    def gen_initial_query_key_value_w(self):\n",
    "        # initializing the weights for Q,K and V\n",
    "        #For multi-head, I don't understand that completely now, but we will come back to it\n",
    "        # self.query_weights = [self.__gen_random_numbers(self.embedding_per_vocab) for _ in range(self.context_len)]\n",
    "        # self.key_weights = [self.__gen_random_numbers(self.embedding_per_vocab) for _ in range(self.context_len)]\n",
    "        # self.value_weights = [self.__gen_random_numbers(self.embedding_per_vocab) for _ in range(self.context_len)]\n",
    "\n",
    "        # For sigle head self-attn layer\n",
    "        self.query_weights = [self.__gen_random_numbers(self.embedding_per_vocab)]\n",
    "        self.key_weights = [self.__gen_random_numbers(self.embedding_per_vocab)]\n",
    "        self.value_weights = [self.__gen_random_numbers(self.embedding_per_vocab)]\n",
    "        \n",
    "\n",
    "    def __gen_random_numbers(self,size):\n",
    "        #not the best way, but it works starting out with any random number\n",
    "        return 2 * np.random.rand(size)\n",
    "\n",
    "    #We need alternating sine cosine position value for each token, but as the frequency increase per couple and need for specific number of couples are\n",
    "    #dependent upon the size of network, we need to set that up\n",
    "    def alternating_sine_cosine_p_value(self,token_position,vector_position):\n",
    "        # token position is like 2 in hello there for there\n",
    "        # vector position is the position of value in first index of first token value\n",
    "        couple_number = math.ceil(vector_position / 2)\n",
    "        # we do (couple_number -1) * frequency change rate, because we don't just multiply by couple number, we have a set frequency increase rate\n",
    "        relative_f_change = couple_number if couple_number == 1 else (couple_number - 1) * self.frequency_change_rate\n",
    "        # we alternate between sine and cosine periodically\n",
    "        sine_or_cosine = 'sine' if vector_position % 2 == 0 else 'cosine'\n",
    "        if sine_or_cosine == 'sine':\n",
    "            return np.sin(np.radians(token_position * (self.base_frequency * relative_f_change)))\n",
    "        else: \n",
    "            return np.cos(np.radians(token_position * (self.base_frequency * relative_f_change)))\n",
    "            \n",
    "    # As the q, k, and v might be needed to be remembered migh as well use MLP class here to give the final matrix\n",
    "    # which should be normalized by passing through final linear layer and get probability distribution\n",
    "    # I don't know the exact way I am going to do, we will tinker and see what works best\n",
    "    # what I am doing here is sending back the whole batch initial embedd ready to be trained one array at a time\n",
    "    def batch_feed_forward_data_ready(self,token_input_arr):\n",
    "        # Passing whole single batch data, means matrix of shape (batch_size * context_len) \n",
    "        # so, weights of every array of batch, multiplies with corresponding weight index/number all at once\n",
    "        token_input_arr = np.array(token_input_arr)[:,np.newaxis,:] * np.array(self.first_weights) # here first_weights are broadcasted to each batch_arr\n",
    "        token_input_arr = token_input_arr.transpose(0,2,1) # we are trying to keep embedding of one voab in single array stream\n",
    "        return token_input_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "53fd07c7-57ee-4a6f-8580-01403d521aa7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This will be the class we would use, which wraps all inner workings with all the previous classes\n",
    "class Model:\n",
    "    def __init__(self,context_len,training_batch_number=None,embedding_len_per_vocab=2):\n",
    "        self.context_len = context_len\n",
    "        self.embedding_per_vocab = embedding_len_per_vocab\n",
    "        self.training_batch_number = training_batch_number\n",
    "        self.attention_layer = MaskedSelfAttLayer(context_len=self.context_len,embedding_len_per_vocab=2)\n",
    "        self.mlp = MLP(nodes_per_layer=12,layers_num=6,embedding_per_vocab=self.embedding_per_vocab)\n",
    "        # Will have error per batch\n",
    "        self.err_per_batch_tokens =  np.zeros(vocab_size)\n",
    "        self.mlp_all_layers_values = []\n",
    "        # This final normalizing layer is used to decrease the parameters of the whole network, rather we use cont_len nodes at the final_layer\n",
    "        # which was incorrect, because we need to create probability distribution over all the possible tokens \n",
    "        # so, final layer will have len(total_tokens) which is vocab_size\n",
    "        self.final_normalizing_layer = Layer(nodes_num=vocab_size,bias=False,prev_layer=self.mlp.layers[self.mlp.layers_num - 1])\n",
    "\n",
    "\n",
    "    def negative_log_loss_penalty(self,ground_truth,predicted_value):\n",
    "        lower_limit = 0.0000001\n",
    "        # lower_limit = 0.8\n",
    "        arr = np.clip(predicted_value,lower_limit, 1-lower_limit)\n",
    "        neumerator = arr - ground_truth\n",
    "        denominator = arr * (np.ones(vocab_size) -  arr)\n",
    "        return neumerator / denominator\n",
    "        \n",
    "\n",
    "\n",
    "    # Need to create Gradient descent values for all of these layers\n",
    "    # calculating the value change that would propagate from each node to the final output, and how changes in input change final output\n",
    "    def backpropagation(self):\n",
    "        #calculate gradient for each layer by muliplying a from last layer to first a being a * w + b = f(x)\n",
    "        self.gradient_last_to_first_layer = []\n",
    "\n",
    "        # Now reversing the values of layers as we are calculating gradient from last to first\n",
    "        # from normalized layer to first layer\n",
    "        reversed_layerWise_values = self.average_per_layers_value[::-1]# clipped the last value\n",
    "\n",
    "        # print(reversed_layerWise_values)\n",
    "        # we leave the last layer value as it is only useful for getting the expected answer and penalty\n",
    "        # In backprop we only use values from second last layer\n",
    "        layers_reversed = self.mlp.layers[::-1]\n",
    "\n",
    "        # This is wrong, every node has different gradient\n",
    "        # Now revising the approach\n",
    "        for i,values_row in enumerate(reversed_layerWise_values):\n",
    "            # print(f'values_row::{reversed_layerWise_values}')\n",
    "            # here we do upto second layer as we don't need first layer values, as the first layer value is just output from\n",
    "            # self-attention layer\n",
    "            if i == 0:\n",
    "                self.gradient_last_to_first_layer.append(values_row)\n",
    "            else:\n",
    "                # gradient_last_to_first_layer[i-1] * values_row * numnber of nodes in layer before this\n",
    "                \n",
    "                # This is the wrong approach\n",
    "                # self.gradient_last_to_first_layer.append(self.gradient_last_to_first_layer[i-1] * values_row * layers_reversed[i - 1].nodes_num)\n",
    "\n",
    "                # Now improving to correct one\n",
    "\n",
    "                for index,val in enumerate(self.gradient_last_to_first_layer[i - 1]):\n",
    "                    # print(f'overflow value::{values_row * val * layers_reversed[i - 1].nodes_num}')\n",
    "                    self.gradient_last_to_first_layer.append(values_row * val * layers_reversed[i - 1].nodes_num)\n",
    "                    # self.gradient_last_to_first_layer.append(values_row * val)\n",
    "\n",
    "\n",
    "        # In this matrix first value of gradient is just for the extra nmormalized layer and all the other 72 array in base model is for\n",
    "        # other nodes in the MLP layers, every nodes weight with different/unique gradient\n",
    "        \n",
    "        # print(f'gradients::::{self.gradient_last_to_first_layer}')\n",
    "        # print(f'length of gradients::{len(self.gradient_last_to_first_layer)}')\n",
    "        # print(f'first_scaled_layer_grad::{self.first_scaled_layer_grad}')\n",
    "\n",
    "\n",
    "    # Update weight and biases, right now I am just updating weights, we will see about biases update later\n",
    "    def gradient_descent(self,lr):\n",
    "\n",
    "        # for standard model settings first array element is for normalized layer, and others leading from back layers to front\n",
    "\n",
    "        layer_current = 0\n",
    "        current_node = 0\n",
    "        \n",
    "        for idx,gradient_box in enumerate(self.gradient_last_to_first_layer):\n",
    "            if idx == 0:\n",
    "                # Here we use this gradient box to tune weights for nodes in normalized layer\n",
    "                for pos,nm_node_weights in enumerate(self.final_normalizing_layer.nodes):\n",
    "                    # weight update rule:: weight = weight - learning_rate * penalty or NLL * gradient\n",
    "                    nm_node_weights = nm_node_weights - gradient_box * lr * self.err_per_batch_tokens[pos]\n",
    "            else:\n",
    "                # Here we tune the weights for all the normal MLP layers\n",
    "\n",
    "                # To know the exact layer we are tunning\n",
    "                # By the way we are not tunning from 1st layer to last, we are doing last to first, so 1 really means layers_num - layer_current\n",
    "\n",
    "                # here we substract 1 first, as the indexing is 0 based\n",
    "                # for _,weight_per_node in enumerate(self.mlp.layers[self.mlp.layers_num - 1 - layer_current].nodes):\n",
    "\n",
    "                #     print(f'weight_per_node::{weight_per_node}')\n",
    "                #     print(f'gradient_Box::{gradient_box}')\n",
    "                #     print(f'gradient_box_index::{idx}')\n",
    "                #     weight_per_node = weight_per_node - gradient_box * lr * self.mlp_error_val\n",
    "                #     print(f'current_layer::{layer_current}')\n",
    "\n",
    "                self.mlp.layers[self.mlp.layers_num - 1 - layer_current].nodes[current_node] -= gradient_box * lr * self.mlp_error_val\n",
    "                current_node += 1\n",
    "\n",
    "                # Here I forgot to increase the gradient_box index, but it seems like I can't do the same way I would in other language\n",
    "                # So now correcting       \n",
    "\n",
    "                if current_node == (self.mlp.layers[self.mlp.layers_num - 1 - layer_current].nodes_num):\n",
    "                    current_node = 0\n",
    "                    layer_current += 1\n",
    "                \n",
    "    \n",
    "    # Creating a abstraction out of train, just to make code more readable and managable\n",
    "    def train_per_batch_arr(self,arr_data, expected_output):\n",
    "        self.attention_layer.setup_main_self_attn_work(arr_data)\n",
    "        batch_single_arr_after_norm = []\n",
    "        # Now everything is setup, now we need to serially go over token embeddings and multiply query with previous tokens key and create a softmax\n",
    "        # between its query key product value and other previous tokens value and aggregate to create final masked self attn-value\n",
    "        for i,_ in enumerate(arr_data):\n",
    "            \n",
    "\n",
    "            if(i == 0):\n",
    "                data_through_mlp, values_across_layers = self.mlp.feed_forward(self.attention_layer.context_history['value'][i]) # where i is always 0 in this situation\n",
    "            else:\n",
    "                # [:(i + 1)] because slicing is exclusive operation that leaves last index\n",
    "                query_key_mmul = np.array(self.attention_layer.context_history['query'][i]) * np.array(self.attention_layer.context_history['key'][:(i+1)])\n",
    "                # now adding each array into single number. eg:- [q2*k1 + q2*k1,q2*k2,q2*k2]\n",
    "                # By the way q2 is different in every instance like k1 and k2, i am just signifying the arr data relating to specific cont_len vocab\n",
    "                \n",
    "                # print(f'query_key_mmul:{query_key_mmul}')\n",
    "                softmax_ready_digits = np.sum(query_key_mmul,axis=1) # By the way axis=1 means adding row not column, 0 means column\n",
    "                # Now perform softmax\n",
    "                # print(f'sofmax_ready_digit::{softmax_ready_digits}\\n')\n",
    "                through_softmax = softmax(softmax_ready_digits)\n",
    "                # Now multiplying sofmax value corresonding to value of each cont_len vocab and add according to corresponding index\n",
    "                # print(f'values::{self.attention_layer.context_history['value'][:i+1]}\\n th_smax::{through_softmax[:,np.newaxis]}')\n",
    "                value_through_contx = self.attention_layer.context_history['value'][:i+1] * through_softmax[:,np.newaxis]# broadcasing through all array\n",
    "                final_summed_val = np.sum(value_through_contx, axis=0) # axis=1 because we need to add all vector embedding through corresponding index\n",
    "                # passing through MLP Network\n",
    "                data_through_mlp, values_across_layers = self.mlp.feed_forward(final_summed_val)\n",
    "\n",
    "            # Now passing through final_normalizing layer and push to batch_single_arr_after_norm arr and return back to train to delegate to some\n",
    "            # abstraction to handle remaining work\n",
    "            final_data = self.final_normalizing_layer.forward_pass(data_through_mlp)\n",
    "            # Also puhsing values across layers into self.mlp_allLayers_value so, that we could calculate average gradient per position and\n",
    "            # also pass through update chain to take multiple data units into consideration with average penalty\n",
    "            self.mlp_all_layers_values.append(values_across_layers)\n",
    "            # print(f'final_data_before_softmax::{final_data}\\n')\n",
    "            output_data = softmax(final_data)\n",
    "            \n",
    "                    \n",
    "            \n",
    "            # print(f'final_data:{final_data}\\n')\n",
    "            # print(f'first_index_val::{final_data[0]}')\n",
    "            batch_single_arr_after_norm.append(output_data)\n",
    "\n",
    "\n",
    "        # We will first calculate the negative log error for each value and then, average them all in a single dim array\n",
    "\n",
    "        for i,arr in enumerate(batch_single_arr_after_norm):\n",
    "            #Create a single dim array with zero initialized array of size vocab_size as we copare to ground_truth and predicted truth\n",
    "            zeros_arr = np.zeros(vocab_size)\n",
    "            # print(f'expected_output[i]__i::{len(arr)}\\n')\n",
    "            # print(f'expected_output[i]::{len(expected_output)}\\n')\n",
    "            zeros_arr[expected_output[i]] = 1\n",
    "\n",
    "            # Now Zeros_arr is the ground truth data arr and arr is predicted data arr, now we need to calculate negative log error for\n",
    "            # for each position\n",
    "            self.err_per_batch_tokens += self.negative_log_loss_penalty(zeros_arr,arr)\n",
    "            \n",
    "            # for j, GT in enumerate(zeros_arr):\n",
    "            #     self.err_per_batch_tokens[j] += self.negative_log_loss_penalty(GT, arr[j])\n",
    "\n",
    "        \n",
    "        # finally return all the batch_single array prediction through softmax\n",
    "\n",
    "        \n",
    "        # print(f'values_per layers in arr 8, len::{len(self.mlp_all_layers_values)}')\n",
    "        return batch_single_arr_after_norm\n",
    "            \n",
    "        \n",
    "    \n",
    "    # batch data and their respective expected output\n",
    "    def train(self,batch_data, batch_e_output,lr=0.001):\n",
    "        self.err_per_batch_tokens = np.zeros(vocab_size)\n",
    "        self.mlp_all_layers_values = []\n",
    "        batch_final_after_softmax = []\n",
    "        batch_predicted_token = []\n",
    "        batch_predicted_token_prob = []\n",
    "        batch_complete_data =self.attention_layer.batch_feed_forward_data_ready(batch_data)\n",
    "        # Now we loop through train_per_batch_arr as function name suggests, and finally get normalized data arr with probability distribution\n",
    "        for i,data in enumerate(batch_complete_data):\n",
    "            train_per_batch_val = self.train_per_batch_arr(data, batch_e_output[i])\n",
    "            for el in train_per_batch_val:\n",
    "                batch_predicted_token.append(np.argmax(el))\n",
    "                batch_predicted_token_prob.append(el[np.argmax(el)])\n",
    "            batch_final_after_softmax += train_per_batch_val\n",
    "\n",
    "        # dividing self.err_per_batch_tokens the number i.e. context_len * batch_size \n",
    "\n",
    "        self.err_per_batch_tokens /= (self.context_len * self.training_batch_number)\n",
    "\n",
    "        # Now averaging all the a value per layer, throughout the whole training process including normalizing layer\n",
    "        # print(f\"self.mlp_allLayers_value::{len(self.mlp_all_layers_values[0][1])}\")\n",
    "        \n",
    "        ############## np.sum does not work for heterogeneous size, so need to use good old loop addition \n",
    "        # self.mlp_all_layers_values = np.sum(self.mlp_all_layers_values,axis=0)\n",
    "        \n",
    "        # Average all samples of value per layer\n",
    "        # it's more like sum of all itterations, cause we haven't divided by total number of itteration count, those beong\n",
    "        # context_len * batch_number\n",
    "        self.average_per_layers_value = []\n",
    "        \n",
    "        for i in range(self.mlp.layers_num + 1):\n",
    "            # Creates zeros array of length equal to the specific layer value length\n",
    "            average_one_layer = np.zeros(len(self.mlp_all_layers_values[0][i]))\n",
    "            for j in range(len(self.mlp_all_layers_values)):\n",
    "                # here we loop through samples number so, j at start and i specifies the layer we are targeting\n",
    "                average_one_layer += self.mlp_all_layers_values[j][i]\n",
    "\n",
    "            average_one_layer /= self.context_len * self.training_batch_number\n",
    "            self.average_per_layers_value.append(average_one_layer)\n",
    "\n",
    "        # print(f'len of averaged up values array:::{len(self.average_per_layers_value)}\\n')\n",
    "        # print(f'len of all layers values:::::{len(self.mlp_all_layers_values)}')\n",
    "        # print(f'len of sel.err len:::::{len(self.err_per_batch_tokens)}')\n",
    "\n",
    "        # setting up error for layers other than normalization layer\n",
    "        self.mlp_error_val = np.sum(self.err_per_batch_tokens, axis=0)\n",
    "\n",
    "        print(f'the final mlp error value:::{self.mlp_error_val}')\n",
    "        # print(f'the final mlp error value len:::{}')\n",
    "        \n",
    "        print(batch_predicted_token)\n",
    "        print(f'predicted_token_prob:{batch_predicted_token_prob}\\n')\n",
    "        print(f'Error_final::{self.err_per_batch_tokens}')\n",
    "        # print(f'the final mlp error value len:::{len(self.err_per_batch_tokens)}')\n",
    "        # print(f'vocab size::{vocab_size}')\n",
    "        # print(batch_final_after_softmax)\n",
    "\n",
    "        self.backpropagation()\n",
    "        self.gradient_descent(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d12869fd-cddc-4a02-afb6-8b82f623b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create batches of training and validation data and also seperate test data, as we donot want the model to remeber, rather\n",
    "#learn patterns\n",
    "encode_ready_tokens = encode_text_assist(final_text)\n",
    "ready_tokens_len = len(encode_ready_tokens) \n",
    "train_text = encode_ready_tokens[:int(ready_tokens_len * 0.9)]\n",
    "validation_text = encode_ready_tokens[int(ready_tokens_len * 0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "6239e0d1-61de-4dad-ad92-81aa20d88b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the final mlp error value:::13849.006648465991\n",
      "[9731, 9731, 9731, 9731, 9731, 9731, 13149, 9731, 13149, 13149, 9731, 13149, 9731, 9731, 9731, 9731, 9731, 9731, 13149, 9731, 13149, 13149, 13149, 13149, 9731, 9731, 9731, 9731, 9731, 9731, 13149, 9731, 3225, 3225, 9731, 3225, 9731, 9731, 9731, 9731, 9731, 4474, 4474, 9731, 3225, 3225, 9731, 1600]\n",
      "predicted_token_prob:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Error_final::[-2.70833260e+06 -8.33332417e+05 -2.08332354e+05 ...  1.00000010e+00\n",
      "  1.00000010e+00  1.00000010e+00]\n",
      "the final mlp error value:::13849.006648463197\n",
      "[10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985, 10985]\n",
      "predicted_token_prob:[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Error_final::[-4.37499944e+06 -6.24999062e+05  1.00000010e+00 ...  1.00000010e+00\n",
      "  1.00000010e+00  1.00000010e+00]\n"
     ]
    }
   ],
   "source": [
    "# print(encode_ready_tokens[:100])\n",
    "batch_size = 4\n",
    "def create_batches(context_len,number_of_batches):\n",
    "    #8 being context_len and we actually randint(0,len(final_text) - context_len + 1) if randint were inclusive but it's not so fine with,\n",
    "    # - context_len+1 as for eg randint(0,8) will give value up to 7 \n",
    "    # np.random.randint(0,len(final_text) - 8, size=4)\n",
    "    random_pick = np.random.randint(0,len(train_text) - context_len,size=number_of_batches)\n",
    "    train_data_batch = [encode_list(train_text[i:i+context_len]) for i in random_pick]\n",
    "    test_data_batch = [encode_list(train_text[i+1:i+context_len+1]) for i in random_pick]\n",
    "    \n",
    "    #return training_data, and each subsequent output as expected prediction,eg:\n",
    "    # train_data_batch = [[1,2,3,4,5],[---]], and xb = [[2,3,4,5,6],[---]], so here context_len is 5 and each index in xa would have it's expected \n",
    "    #output in xb\n",
    "    return train_data_batch, test_data_batch\n",
    "\n",
    "# This data kinda suck, while having a shorter context-len like 8, so, increased to 12\n",
    "model = Model(12,4)\n",
    "\n",
    "for _ in range(2):\n",
    "    train_data, expected_prediction = create_batches(model.context_len,model.training_batch_number)\n",
    "    model.train(train_data,expected_prediction,lr=0.0000000000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "d67da0a6-a617-4519-abbf-89c94122e89e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 1  4]\n",
      "  [ 1  6]]\n",
      "\n",
      " [[ 2  6]\n",
      "  [ 2  9]]\n",
      "\n",
      " [[ 3  8]\n",
      "  [ 3 12]]]\n",
      "[[[ 1  1]\n",
      "  [ 4  6]]\n",
      "\n",
      " [[ 2  2]\n",
      "  [ 6  9]]\n",
      "\n",
      " [[ 3  3]\n",
      "  [ 8 12]]]\n",
      "[0.65900114 0.24243297 0.09856589]\n",
      "[[1], [2], [3], [4], [5]]\n",
      "[2 3 4 5 6]\n",
      "log_softmax::[    0.  -999. -2000.]\n",
      "-100000000.0\n",
      "[[3 6]\n",
      " [4 6]\n",
      " [5 6]]\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[1, 2], [2, 3], [3, 4]])\n",
    "B = np.array([[1, 2], [1, 3]])\n",
    "\n",
    "result = A[:, np.newaxis, :] * B  # Broadcasting\n",
    "print(result)\n",
    "print(result.transpose(0,2,1))\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x))  # Subtract max(x) for numerical stability\n",
    "    return exp_x / np.sum(exp_x)\n",
    "\n",
    "# Example usage:\n",
    "x = np.array([2.0, 1.0, 0.1])\n",
    "softmax_values = softmax(x)\n",
    "print(softmax_values)\n",
    "\n",
    "\n",
    "a = np.array([1, 2, 3, 4])  # Shape: (4,)\n",
    "b = np.array([[1, 2, 3, 4],  # Shape: (4, N)\n",
    "              [1, 2, 3, 4],  \n",
    "              [1, 2, 3, 4],  \n",
    "              [1, 2, 3, 4]])\n",
    "\n",
    "result = b * a[:, np.newaxis]  # Expanding `a` to (4, 1) and broadcasting\n",
    "# print(result)\n",
    "\n",
    "# print(b * np.array([[1],[2],[3],[4]])) # same as what we did before with np.newaxis\n",
    "\n",
    "print([[1],[2],[3]] + [[4],[5]])\n",
    "\n",
    "a = np.array([[1.13645561e-02, 1.52903492e-02],\n",
    " [4.53067962e+01, 6.55301006e+01],\n",
    " [6.57354624e-03, 1.17096469e-01]])\n",
    "np.sum(a,axis=1)\n",
    "print(np.array([1,2,3,4,5]) + np.array([abs(-1)]))\n",
    "\n",
    "x = np.array([1000, 1001, 1002])\n",
    "x_max = np.max(x, axis=-1, keepdims=True)  # Find max value (1002)\n",
    "x_stable = x - x_max  # Subtract 1002 from all elements â†’ [-2, -1, 0]\n",
    "\n",
    "exp_x = np.exp(x_stable)  # Now, exp(x_stable) = [e^(-2), e^(-1), e^0]\n",
    "# softmax = exp_x / np.sum(exp_x)\n",
    "# print(softmax)  # Outputs stable probabilities\n",
    "\n",
    "def log_softmax(x, dtype=np.float32):\n",
    "    x = np.asarray(x, dtype=dtype)\n",
    "    x_max = np.max(x, axis=-1, keepdims=True)\n",
    "    log_sum_exp = np.log(np.sum(np.exp(x - x_max), axis=-1, keepdims=True))\n",
    "    return x - x_max - log_sum_exp\n",
    "\n",
    "print(f'log_softmax::{log_softmax(np.array([1000, 1, -1000]))}')\n",
    "\n",
    "def negative_loss_loss_penalty(ground_truth,predicted_value):\n",
    "        lowest_possible_value = 0.00000001\n",
    "        clipped_val = np.clip(predicted_value, lowest_possible_value, 1-lowest_possible_value)\n",
    "        return (clipped_val - ground_truth)/(clipped_val * (1 - clipped_val))\n",
    "\n",
    "print(negative_loss_loss_penalty(1,0))\n",
    "\n",
    "\n",
    "arr = np.array([[[1,2],[2,2],[3,2]], \n",
    "                [[1,2],[1,2],[1,2]], \n",
    "                [[1,2],[1,2],[1,2]]])\n",
    "\n",
    "result = np.sum(arr, axis=0)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ac7c1f-1057-4ef7-99e3-0d22e84c54f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
